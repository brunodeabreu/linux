[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "linux",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Sysadmin1.html#what-is-linux",
    "href": "Sysadmin1.html#what-is-linux",
    "title": "3  Server Administrator I - RH124",
    "section": "3.1 What is linux ?",
    "text": "3.1 What is linux ?\nModular operations system where you can add components :\n\n\n\n\n\nOpen Source\n\nCopyleft lic\n\nGNU : General Public License\nLGNU: Lesser General Public License\n\nPermissive lic:\n\nMIT\nSimplified BSD\nApache Software License\n\n\nRed Hat contribute and facilitate open source projects, validating the code and support code that Red Hat validate and test.\nSample of Red Hat Products:\n* Red Hat Enterprise Linux 8\n* Red Hat Open Shift 4\n* Red Hat Ansible Automation\n* Red Hat Ceph Storage\n* Red Hat OpenStack Platform\n* Red Hat Virtualization\n* Red Hat Gluster Storage\nBenefits of open source software:\n\nCode can survive the loss of orginal developer or distributor\nWe can learn from real-world code and develop more effective applications\nRed Hat sponsor and integrate open source projects into the Fedora project and participate in upstream projects"
  },
  {
    "objectID": "Sysadmin1.html#command-line",
    "href": "Sysadmin1.html#command-line",
    "title": "3  Server Administrator I - RH124",
    "section": "3.2 Command Line",
    "text": "3.2 Command Line\n\n3.2.1 $ and #\nAttention on shell when you are on command line:\n\nRoot user : #\nNormal user : $\n\nSample of using - or --\n-all : Shell going to be interpreting all arguments individually \n\n# Shell going to be interpreting all arguments individually  a, l and l\nls -all . \n\n--all : Shell will interpreting the entire word\n# Shell will interpreting the entire word\nls --all .\n\n\n3.2.2 To login into another computer using ssh\nssh &lt;machine or server&gt;\n\n\n3.2.3 Executing commands\n\n# List the user that you  login\nwhoami\n\n# formatting the dates\ndate +%F\n\n# Executing one command after the other\ndate +%A; uname -r; whoami\n\n\nConditional commands execution &&\n\nThis is the basic IF statement\n\nThe date command must complete successfully than the uname -r command will be running and if execute successfully the whoami command will be executed\n\n# If with and\ndate +%F && uname -r && whoami\n\necho \necho\n# If with or\nate +%F || uname -r && whoami\n\ncommand history\n\n!! : execute the last command\n!12: execute the command 12 on history\nctr+r \n\ncheck type of file\nThe file command is used to determining the type fo file :\nfile /etc/issue\n\necho \necho\n\nfile /bin/ \ncheck content of file\n\ncat or tac\nhead or tail\nwc -l : count lines of file"
  },
  {
    "objectID": "Sysadmin1.html#managing-file-system",
    "href": "Sysadmin1.html#managing-file-system",
    "title": "3  Server Administrator I - RH124",
    "section": "3.3 Managing File system",
    "text": "3.3 Managing File system\n\n\n\n\n\n\n3.3.1 The file system hierarchy /\n\nTry command tree -d\n\n\n/ : Top of system file system hierarchy\n/usr : unix system resources, installed software and libraries\n\n/usr/bin : regular commands and utilities\n\n/bin : binaries executable that are usable by normal users\n/sbin : system binaries executable typically used by root user\n/boot : component that are necessary to boot file system, like bootloader called grub 2 and linux kernel\n/dev : device files, represent hardware components\n/etc : extended text configuration configuration files\n/home : home dir for normal users\n/run : runtime data going to be recreate on a reboot\n/var: variable data that should survive a reboot, log files, database and website files\n/root : root user home dir\n/tmp : accommodate temporary files, deleted by 10 days\n/var/tmp : another temp dir , purge every 30 days\n\n\n\n3.3.2 Absolute and Relative paths\n\nAbsolute path, is the complete path, no ambiguity, start with / :\n\n#sample \ncat /etc/issue\n\nRelative path is the path to a file relative the current position, do not start with /\n\ncd /\ncat etc/issue\n\n\n3.3.3 Managing Files\n\nmkdir -p : create the parents that do not exists\n\npwd\nmkdir -p dirA/dirB/dirc\nls -R dirA\nCommands\n\ncp -r : copy dir\nrm -i : show to you msg to confirm\nrmdir : remove dir\nrm -r : delete dir recursive\nmv : move\ntouch : create files\n\nLinks\nIndex Node ((inode_): How files are identified, keep track of: * permissions * ownership * date & time stamps * paths to data on file system\nls -li LICENSE\n\n40652348 : file ID (inode)\n1 : means one name using this iNode right now\n\nHard Link\nln LICENSE LIC2\nls -li LIC*\n\n\nBoth LIC2 and LICENSE have the same iNode but the number right now is 2, not able to identify which one was create first, this is a hard link\n\nSoft Link\nln -s README.md README_2.md\nls -li READ*\n\nBoth have different iNode number, so they are two different files, the permission there are a l meaning link, this is a soft link, if we delete file 3 we have a broken link\n\n\n# create tmp files\nmkdir -p tmp ;cd tmp ;touch 1file 2file 3file 4file able alfa baker bravo cast easy echo _src\n\n# List files that match this case\necho\necho \"List files that match this case Xfile\"\nls ?file\necho \"----\"\n\n# list files that begin with a or c or e \necho\necho \"List files that begin with a or c or e \"\nls [ace]*\necho \"----\"\n\n\n# List files that do not start with a c or e\necho\necho \"List files that do not start with a c or e\"\nls [^ace]*  # or ls [!ace]*\necho \"----\"\n\n\n# List all file that begin with an alphabetical character\necho\necho \"List all file that begin with an alphabetical character\"\nls [[:alpha:]]*\n\n# List all files that begin with digit\necho\necho \"List all files that begin with digi\"\nls [[:digit:]]*\necho \"----\"\n\n# List all files that match digit or alphabetical char\necho\necho \"List all files that match digit or alphabetical char\"\nls [[:alnum:]]*\necho \"----\"\n\n\n# List all files that begin with punctuation \necho\necho \" List all files that begin with punctuation\"\nls [[:punct:]]*\necho \"----\"\n\n# clean\ncd ..\nrm -rf tmp\nBrace expansion\nmkdir -p tmp ;cd tmp\necho {Sun,Mon,Tues,Wednes}day.log\n\n# create dirs\nmkdir -p RHEL{6,7,8}; ls RHEL*\n\n# create sequence of files using ..\ntouch song{1..5}.mp3 ; ls *.mp3\n\n#clean\ncd ..\nrm -rf tmp\nVariable\nSOMETHING=value\necho $SOMETHING\n\n\n# command substitution\necho \"Today is $(date +%A)\"\n\n\nskell\nThe contenct of path /etc/skel is automatically copied to all users"
  },
  {
    "objectID": "Sysadmin1.html#help",
    "href": "Sysadmin1.html#help",
    "title": "3  Server Administrator I - RH124",
    "section": "3.4 Help",
    "text": "3.4 Help\nAll man pages are on /usr/share/man\nCommands\n\nman\npinfo\n\nman -k cron\nTo go direct to a section , belo example of command to go direct to session 5\nman 5 crontab"
  },
  {
    "objectID": "Sysadmin1.html#text-files",
    "href": "Sysadmin1.html#text-files",
    "title": "3  Server Administrator I - RH124",
    "section": "3.5 Text Files",
    "text": "3.5 Text Files\n\n\n\n\n\nChannels\n\n0 : stdin , read only used by keyboard\n1 : stdout, write only, terminal\n2 : stderr, write only, terminal\n3+ : file name, read and write\n\nWe can direct errors to a specific file like below I do not have the file or dir abc\nls abc file1 2&gt; errors.log\n\n# output\nfile1\n\n# on errors.log\nls: cannot access 'abc': No such file or directory\n\n\nWe also can send both to the same file\nls /show /boot &&gt; combine.log\n# another option \n# ls /show /boot &gt; combine.log 2&gt;&1\n\ncat combine.log\nrm combine.log\n\nAnother usage when we have an output with lot of errors and can only the the results, all the errors message was sent to /dev/null\nfind / -iname passwd 2&gt; /dev/null\n\n#output\n/usr/share/ -completion/completions/passwd\n/usr/share/doc/passwd\n/usr/share/lintian/overrides/passwd\n/usr/bin/passwd\n/etc/passwd\n/etc/pam.d/passwd\nA good usage is send successfully output to an output.log file and the errors to errors.log file\nls /shoe /boot &gt;&gt; output.log 2&gt;&gt; errors.log\n\nSend emails with content of file, the &lt; will direct the content of file to email to &lt;user&gt;\nmail -s \"Subject text\" &lt;user&gt;  &lt; file\nPipe\nThe resolv.conf have 6 lines in total but we can see the lines without comment # and direct the output using pipe |\nwc -l /etc/resolv.conf\n\necho\n\n# check the lines without comment\ngrep ^[^#] /etc/resolv.conf\n\necho\n# combine the grep with pipe\ngrep ^[^#] /etc/resolv.conf | wc -l\n\n\n\nAnother way is get the output and save on file using pipe | and tee\nfind / -iname passwd 2&gt; /dev/null | tee find.out\n\n\n3.5.1 vim\nModes :\n\nInsert : i\nCommand : default\n\nyy : Copy\np : paste\n5p : paste 5 times\ndd : delete\nZZ : save and quite\ncw : change word\nx : delete character\nr : replace character\na : append\n\nExtend command : :\n\nq! : exit without save\nwq : save and exit\n\nVisual : v\n\ncrt + v : block column to manipulate column\nshift + v: line mode to select line\nx : delete\nu : undo\n\n\n\n\n3.5.2 Changing shell\n\nSetting a editor\n\n# To set\nenv EDITOR=nano crontab -e\n\n# To unset\nexpor -n EDITOR\n\nThe crotab will open on nano editor instead of vi or vim\nAlso can set the variable EDITOR to nano and the same will hapens if we call crontab é\n# using export\nexport EDITOR=nano\nSome . * files and adjust\n\nAll the history are saved on . _history\n. rc user specific env for example:\n\nwe can add more lines on history add the variable export HISTFILESIZE=2000\nadd export HISTTIMEFORMAT=\"%F %T \""
  },
  {
    "objectID": "Sysadmin1.html#managing-local-users-and-groups",
    "href": "Sysadmin1.html#managing-local-users-and-groups",
    "title": "3  Server Administrator I - RH124",
    "section": "3.6 Managing Local Users and Groups",
    "text": "3.6 Managing Local Users and Groups\n\nwhoami : Show your user\nid : show your user, groups\nType of users*\n\nSuper User root\nAccount users, not used by people\nRegular users\n\n\n\n3.6.1 Users\n\n/etc/passwd : file with user info\n\ngrep bruno /etc/passwd\n\n#output\nbruno:x:1000:1000:bruno,,,:/home/bruno:/bin/ \n\nUser : Bruno\nx : long time ago where password where stored\n1000 : User ID\n1000 : group ID\nComment\nUser home dir : /home/bruno:/bin/\nThe password is stored on /etc/shadow, only root can access this file\n\nSample : 6$CSsXcYG1L/4ZfHr/$2W6evvJahUfzfHpc9X.45Jc6H30E...output omitted..\n\n6 : Hash algorithm used , SHA-512\nCSsXcYG1L/4ZfHr/ Used by cript info\nW6evvJahUfzfHpc9X.45Jc6H30E has password\n\n\nAnother way to list and return user info is using getent command\n\ngetent passwd bruno\n\n#output\nbruno:x:1000:1000:bruno,,,:/home/bruno:/bin/ \n\nuseradd : create new user, sample : useradd kano\nuserdel : remove user\n\nsample 1: userdel kano, but do not remove the data and home dir\nsample 2: userdel -r kano, delete user and data/home dir\n\nusermod -c : add comments to user\n\nusermod -a -G &lt;user&gt;\n\n-a : append to the secondary group membership, if do not use a overwritten the current info\n-G : group name\n\nusermod -g &lt;group&gt; &lt;user&gt;\n\n-g : change the primary group info\n\n\nusermod -L &lt;user&gt; : Lock a user account\nusermod -U &lt;user&gt; : Unlock a user account\n\n\n\n3.6.2 Groups\n\n/etc/group : file with group info\n\ngrep bruno /etc/group \n\n#output\nShow in New Window\n/etc/issue: ASCII text\n\n\nadm:x:4:syslog,bruno\ncdrom:x:24:bruno\nsudo:x:27:bruno\ndip:x:30:bruno\nplugdev:x:46:bruno\nlpadmin:x:113:bruno\nbruno:x:1000:\nsam are:x:130:bruno\n\nbruno : group name\nx : pwd info that is not used\n1000 : group id\nMembers : of group\n\n\nShow groups from my users using command groups.\n\n\ngroupadd : add new group (GID &gt; 1000)\n\n-g : specify the GID, groupadd -g 1000 group1\n-r : create the system group (GID 0-999)\n\ngroupdel : remove group groupdel &lt;group_name&gt;\ngroupmod : modify the group\n\n-n : change the group name, groupmod -n &lt;new_name&gt; &lt;old_name&gt;\n\n\n\n\n3.6.3 Gaining superuser access\n\nsu : change to root super user, without set the profile, the path still your regular user\nsu - : change to root user and set the profile and env of root\nsudo : allow run commands as another user\nsudo visudo : sudoers file\n\nLine : if you are member the group wheel you can be logged in from any computer and you can run all cmds as all users\n\n# Allows people in group wheel to run all commands\n%wheel  ALL=(ALL) ALL  \n\n% indicate a group\n\nsudo -i or sudo su - : assume as root user\n/etc/sudoers.d/&lt;user&gt; : drop config into that on dir /etc/sudores.d as if you are editing the /etc/sudoers\n\nTip\nTo create /etc/sudoers.d/admin file and grant all members of admin group total privileges\necho \"%admin ALL=(ALL) ALL\" &gt;&gt; /etc/sudoers.d/admin\nTo create just for one user\necho \"user ALL=(ALL) ALL\" &gt;&gt; /etc/sudoers.d/user\n\n\n\n3.6.4 Managing user passwords\nWe no longer store password on /etc/passwd, the password are stored on /etc/shadow\n\nchage : used to change the aging info for a user password\n\nSample : chage -m 1 -M 26 -W 4 -I 3 -E (2019-05-31) &lt;user&gt;\n\n-m : min wait time before user can change the pwd again, in this case 1 day\n-M : when they have to changed the pwd, in this case in 26 days need to change\n-W : warning of 4 days\n-I : once the pwd expire they have 3 more days to login and change the pwd\n-E : expiration date\n\n\nchage -l : show all the info\n/etc/login.defs : Define info of login such PASS_MAX_DAYS, PASSMIN_DAYS, etc\nAdd user with nologin shell\n\nuseradd -s &lt;user&gt; -s /sbin/nologin\n\n\nTip\nChange the user and update expiration to more 180 days\nchage -E $(date -d + 180days +%Y-%m-%d) &lt;user&gt;"
  },
  {
    "objectID": "Sysadmin1.html#controlling-access-to-files",
    "href": "Sysadmin1.html#controlling-access-to-files",
    "title": "3  Server Administrator I - RH124",
    "section": "3.7 Controlling access to files",
    "text": "3.7 Controlling access to files\ndrwxrwxrwx\n\nd : dir, can be l link, etc\nr | 4 : read\nw | 2 : write\nx | 1 : execute\n\nOwning User : first 3 rwx Owning Group : next 3 Other : last 3\nCommands\n\nchmod : change the permission mod\n\nchmod 740 &lt;file or dir&gt; or\nchmod o+rw &lt;file or dir&gt;\n\nchown : change ownership\n\nchmod user:group &lt;file or dir&gt;\nchmod :group &lt;file or dir&gt; or\nchmod user &lt;file or dir&gt;\n\n\nTip\nGive you read and execute permission do dir but no execute on files inside the dir, i.e, we can list the content but not execute files inside\nchmod -R a=rX\n\n3.7.1 Special permissions\nSticky bit : In a collaborative dir you can create files on the dir and only delete files that you have created\n\nu+s (suid) : Files executes as the user that owns the file, not the user that ran the file\ng+s (guid) : file execute as the group that owner the file, files newly create in directory have their group owner set to match the group owner of the dir.\no+t (sticky): Users with write access to dir can only remove files that they own\nSticky bit — directories o+t\n\nTo set o+t or 1\n\nchmod o+t &lt;dir&gt; or\nchmod 1770 &lt;dir&gt;\n\nTo remove o-t or 0\n\nchmod o-t &lt;dir&gt; or\nchmod 0770 &lt;dir&gt;\n\n\n\n# To set sticky bit\nchmod o+t dirA\nls -ld dirA\n\n# To remove sticky bit\n#chmod o-t dirA\n\ns : Files created on the dir will have the same owning group of dir\n\nSet grid to dirs or files g+s\n\nTo set g+s\n\nchmod g+s &lt;dir&gt;\n\nTo unset g-s\n\nchmod g-s &lt;dir&gt;\n\n\n\n# To set\nchmod g+s dirA\nls -ld dirA\n\n# To remove \n# chmod g-s dirA\n\nSet uid to files u+s\n\nTo set u+s\n\nchmod u+s &lt;file&gt;\n\nTo unset u-s\n\nchmod u-s &lt;file&gt;\n\n\n\n\n\n3.7.2 Default permissions - umask\n\numask : show umask info\numask 0000 : set the umask to 0000 or 777 to new dirs\n\nTo change the user umask we can update the . rc file\n0022: * The permissions of files going to be 755"
  },
  {
    "objectID": "Sysadmin1.html#monitoring-and-managing-linux-process",
    "href": "Sysadmin1.html#monitoring-and-managing-linux-process",
    "title": "3  Server Administrator I - RH124",
    "section": "3.8 Monitoring and Managing Linux process",
    "text": "3.8 Monitoring and Managing Linux process\nProcess state description\n\n\n\n\n\n\n3.8.1 Commands to monitor\n\ntop\n\n%CPU\nload average\ntasks\n\nps aux , ps -ef\n\nPID : process ID\nPPID: Parent process ID\nTime\nCMD\nTTY : from where the process is running\n\nhtop\n\n\n\n3.8.2 Controlling jobs\nsample : gnome-calculator &\nCommand : * jobs : list all jobs * fg %&lt;job_number&gt; : Bring the job to foreground : * Ctrl+z : suspend/stop the job * ps j : show the info relate jobs * bg %&lt;job_number&gt; : restart job * Ctrl+c : terminate job\n\n\n3.8.3 Killing process\nSignals\nkill -l\n\ndefault kill -15 &lt;process&gt;\ndie right now kill -9 &lt;process&gt;\nstop kill -19 &lt;process&gt;\ncontinue kill -18 &lt;process&gt;\nkill with -15 several with same name killall &lt;name&gt;\npkill -t pts/2 : terminate the user logged on pts/2*\n\npkill -SIGTERM tail : will kill the tail process running\npstree : display tree view of process\npgrep -l -u &lt;user&gt; : identify the process that going to be killed by pkill\n\n\n\n3.8.4 Monitor process activity\nLoad Average\n\nuptime command review the load average\n\nuptime\n\n1,24 : Last min\n1,07 : Last 5 min\n0,93 : Last 15min\n\nTo analyze the load average need to know how many CPU’s do we have\nlscpu | grep -i 'CPU(s)'\n\n#or\necho\necho \"========Number of CPUs=====\" : \ncat /proc/cpuinfo | grep \"model name\" | wc -l\n\n#output\n\nCPU(s):                          8\nOn-line CPU(s) list:             0-7\nNUMA node0 CPU(s):               0-7\n\n========Number of CPUs===== :\n8\n\n\nSo 1,24 / 8 is 0,155 , what means that my CPU is busy 15% of the time on last min\nOn last 15min my cpu is 11% (0,93/8) last 15min and 13%(1,07/8) last 5min.\nTo know if the CPU is overload the results show be 1.13 what means that my CPU is overload by 13% on that particular time\nAnother example\n\n#From lscpu, the system has four logical CPUs, so divide\n#                                   load average: 2.92, 4.48, 5.20\n#divide by number of logical CPUs:                  4     4     4\n                                                   ---- ---- ----\n                          per-CPU load average:    0.73 1.12 1.30\n                              \n# This system's load average appears to be decreasing.\n# With a load average of 2.92 on four CPUs, all CPUs were in use ~73% of the time.\n# During the last 5 minutes, the system was overloaded by ~12%.\n# During the last 15 minutes, the system was overloaded by ~30%\n\ntop is another command that can be used to monitor the system\n\nk : ask by the PID to be terminated\nM : sort by memory\nh : help\nshift + w : write the top config on /home/&lt;user&gt;/.config/procps/toprc\n\nOn top :\n\nPID : process ID\nPR : Priority\nVIRT : Virtual memory that process is using\nRES : Physical RAM used\nSHR : Shared memory\n%CPU\n%MEN\nTime : how long it is running\nCommand"
  },
  {
    "objectID": "Sysadmin1.html#controlling-services-and-daemons",
    "href": "Sysadmin1.html#controlling-services-and-daemons",
    "title": "3  Server Administrator I - RH124",
    "section": "3.9 Controlling Services and Daemons",
    "text": "3.9 Controlling Services and Daemons\nThe systemd is responsable for initializing the system and uses units that represent daemons: * service : database, web service, tc * target : collection of units * device * socket\nsystemctl list-units  | head -n 10\n\n# or\n# systemctl\n# systemctl list-units --type=service\n# systemctl list-units --type=target\n\nstatus\n\nStatus of sshd : systemctl status sshd\n\n#systemctl list-units --type=service\n\nsystemctl status bluetooth.service\n\n\n#output\n\n● bluetooth.service - Bluetooth service\n     Loaded: loaded (/lib/systemd/system/bluetooth.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2021-08-22 08:50:54 -03; 5 days ago\n       Docs: man:bluetoothd(8)\n   Main PID: 906 (bluetoothd)\n     Status: \"Running\"\n      Tasks: 1 (limit: 18970)\n     Memory: 2.7M\n     CGroup: /system.slice/bluetooth.service\n             └─906 /usr/lib/bluetooth/bluetooth\n\nStop : systemctl stop &lt;service&gt;\nStart : systemctl start &lt;service&gt;\nRestart: systemctl restart &lt;service&gt;\nReload : systemctl reload &lt;service&gt;\nask : systemctl is-active &lt;service&gt; or ...is-enable..., is-failed\ndisable: systemctl disable &lt;service&gt;, the service will not start when the system is started\nlist dependency : systemcl list-dependecies &lt;service&gt;\nmask : systemctl mask &lt;service&gt; prevent service to be started\nunmask: systemctl unmask &lt;service&gt;"
  },
  {
    "objectID": "Sysadmin1.html#configuring-and-securing-ssh",
    "href": "Sysadmin1.html#configuring-and-securing-ssh",
    "title": "3  Server Administrator I - RH124",
    "section": "3.10 Configuring and Securing SSH",
    "text": "3.10 Configuring and Securing SSH\nOn RHEL8 we have OpenSSH which implement not only the SSH daemon but also SSH command line tool\n\n~/.ssh/known_hosts stores the fingerprint sent by server to future communication\nTo define the StrictHostKeyChecking we can edit the file ~/.ssh/config or /etc/ssh/ssh_config\n\n\n3.10.1 Configure SSH Key-based Authentication\n\nprivate - decrypt\n\nWhen you connect to ssh server using private key the remote machine will generate a challenge (encrypted with your public key), if you are able to decrypt that encrypted challenge then yo are allowed to make a connection\n\npublic - encrypt\n\nwhen you ssh to a server the public key is saved on the SSH server\n\n\n\n\n3.10.2 Create a public and private keypair\n\nIssue to start and create the Key : ssh-keygen\nEnter the file name\nCreate a paraphrase\n\nResult: We going to have a public and private key created\n\nInstall the key on server\n\nssh-copy-id -i .ssh/&lt;public key name&gt; &lt;hostname&gt;\n\nTest\n\nssh -i .ssh/&lt;public key name&gt; &lt;hostname&gt;\n\nAdd the private key to agent\n\n# start the agent\neval $(ssh-agent)  \n\n# add the key\nssh-add .ssh/&lt;public key name&gt;\n\n\n3.10.3 Customizing OpenSSH Service Config\n\nDisable the root ability to login on /etc/ssh/sshd_config to modify the daemon config and change the PermitRootLogin to no\n\nTecniques to avoid password\n\nCreate a sshusers group and configure /etc/ssh/sshd_config 4 allowGroups, or\nCreate a private key and public key"
  },
  {
    "objectID": "Sysadmin1.html#analysing-and-storing-logs",
    "href": "Sysadmin1.html#analysing-and-storing-logs",
    "title": "3  Server Administrator I - RH124",
    "section": "3.11 Analysing and Storing Logs",
    "text": "3.11 Analysing and Storing Logs\n\nSystemd is the heart of RHEL8 system and need to analyse how it is working\n\nThe journal collects messages from several sources (booting, daemons, etc) and we can query using journalctl but this is not persistent by default.\nThere are another process rsyslog that read syslog and receive systemd-journal and save it on /var/log\nFacility is one of the following keywords : auth, authpriv, cron, daemon, kern, lpr, mail, mark, news, security, syslog, user, uucp, local0 to local7.\nPriority is one of the following keywords : debug, info, notice, warning, warn, err, error, crit, alert, emerg, panic\n\n/var/log/messages : Store most of syslog messages\n/var/log/secure : store syslog messages related to security and authentication operations\nrsyslog : service that organize syslog messages into /var/log\n/var/log : directory of syslog files\n/var/log/maillog : store syslog messages related mail server\n/var/log/cron : store syslog messages related to the schedule jobs\n/var/log/boot.log : store console message related to system startup\n\nSyslog codes\n\n0 - emerg\n1 - alert\n2 - crit\n3 - err\n4 - warning\n5 - notice\n6 - info\n7 - debug\n\nThe message is organized as facility.priority and we can configure to direct it to particular file like authpriv.notice and send to /var/log/foo\nConfig file : /etc/rsyslog.conf, where we can add all rules\n\nAvoid edit the main config file /etc/rsyslog.conf and use drop-in directory /etc/rsyslog.d\n\nLog rotate prune framework\n\n/etc/logrotate.conf : Config file\n/etc/logrotate.d : drop-in dir\n\nManual messages to syslog sample :\nlogger -p local7.notice \"log entry created on host\"\n\n\n\n3.11.1 Reviewing System Hournal Entries\nJournal is not persistent and can find the logs on /run/log/journal\n\nsystemd-journald - command journalctl\n\njournalctl -r : reverse the log message, last page of msg show first\njournalctl -u &lt;unit&gt; : show logs from unit such as sshd.service\njournalctl -u &lt;unit&gt; --since today : show today message of particular unit\njournalctl -u &lt;unit&gt; --since \"2019-04-15 09:00:00\" --until \"2019-04-15 11:00:00\" -p warning : Show the warning message of unit in particular time\njournalctl --since \"2019-04-15 09:00:00\" --until \"2019-04-15 11:00:00\" -p warning : Show ALL warning message in particular time\nsince request all warning we also going to see the err, crit, alert and emerg messages\njournalctl -p err : show the errors\njournalctl -f : to monitor the system\njournalctl -n 50 : to show the last 50 messages\njournalctl _SYSTEM_UNIT=sshd.service _PID=xx : will show to you message about this unit and PID\njournalctl -b -1 : show the data related one boot if the journal is persistent\n\n\n\n\n3.11.2 Preserving the system journal\nBy default the journal is not persistent on /run/log/journal and after reboot we lose all data. To preserve the journal we need to create the dir /var/log/journal, also by default journal are not allow to get more than 10% of file system or leave less than 15% of the file system free. The configuration are on /etc/journald.conf\n\n\n3.11.3 Maitaining Accurate time\n\nHave system in sync is important\n\nChecking time\n\ntimedatectl\n\n#output\n\n               Local time: Fri 2021-08-27 13:24:50 -03   \n           Universal time: Fri 2021-08-27 16:24:50 UTC   \n                 RTC time: Fri 2021-08-27 16:24:50       \n                Time zone: America/Sao_Paulo (-03, -0300)\nSystem clock synchronized: yes                           \n              NTP service: n/a                           \n          RTC in local TZ: no  \n\nTo list the timezones we can use the command timedatectl list-timezones and set with command:\ntimedatectl set-timezone &lt;timezone&gt; \nThe system will use NTP (Network Time Protocol) to synchronize the time with a machine to perform that we can set it to True\ntimedatectl set-ntp true\nRedHat 7 and 8 use the chronyd.service to synchronize the time and inside of /etc/chrony.conf we going to find the list of servers that are in sync and can check the sources with chronyc sources"
  },
  {
    "objectID": "Sysadmin1.html#managing-networking",
    "href": "Sysadmin1.html#managing-networking",
    "title": "3  Server Administrator I - RH124",
    "section": "3.12 Managing Networking",
    "text": "3.12 Managing Networking\nTCP/IP layers :\n\nApplication : How clients communicate across plataforms, sample Web browser talking withWeb server\n\nSSH : remote login\nHTPS : web\nNFS or CIFS : file share\nSMTP : emal\n\nTransport : How packets are sent and received like\n\nTCP : connection-oriented bi-direction form of guaranteed messaging\nUDP : connectionless unidirectional and non-guaranteed messaging\n\nInternet : Specify how packets are routed acrosss network\nLink layers : physical askpects of networking\n\nMAC\n\n\n\n\n\n\n\nDevices :\nThe name dependes on where and how the device is connected\n\nEN : Ethernet devices\n\nENO1 : Onboard Ethernet Interface with index number 1\nENS3 : Ethernet device in hotplug slot 3\n\nWLAN : Wireless LAN devices\nWWAN : Wireless AN devices\nWLP4S0 : PCI buss number 4 connect on slot 0\n\n\n3.12.1 TCPIP\nIPV4\n\nIP\n\nIP Address : 32bits divided up into 4 octets, this is the interface unique identity on the network\nSubnet mask : divide the IP address into the host portion as well as the network portion and that is used to facilitate routing\n\nNotation : 255.255.255.0 or /24\n\n\n\nIPV6\n“New implementation of IP” : Allow every single person to have its own unique IP Address\n\nIPV6 is 128-bit number with 8 colon separated groups of 4 hexadecimal nibbles, also use subnet mask and normally make use of /64, i.e, 64 bits\n\n\n\n\n\n\n\nCommon ipv6 address\n\n::1/28 : localhost\n::/0 : default route\nfe80::/10 : link-local (start with fe80), those link are not routable, when we have IPV6 enable the link-local will be allocated automatically and allow to communicate with other machines using IPV6 on the same local segment\n\n\nip a\n\n#output\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp3s0f1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000\n    link/ether 80:fa:5b:4d:16:6d brd ff:ff:ff:ff:ff:ff\n3: wlp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether f8:94:c2:74:dc:04 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.15.8/24 brd 192.168.15.255 scope global dynamic noprefixroute wlp4s0\n       valid_lft 35492sec preferred_lft 35492sec\n    inet6 fe80::19da:6be7:d851:7ea9/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n\n\n\nThe IPV6 : inet6 fe80::19da:6be7:d851:7ea9/64 scope link\nInterface : enp3s0f1\nMAC : link/ether 80:fa:5b:4d:16:6d brd ff:ff:ff:ff:ff:ff\nIPV4 : inet 192.168.15.8/24 brd 192.168.15.255 scope global dynamic \n\nCommands\n\nPING tool for IPV6 : ping6 &lt;ipv6&gt;%&lt;interface&gt;\nShow IPv4 table : ip route\nShow IPv6 table : ip -6 route\nShow specific interface : ip a s &lt;interface\nProperties of interface : ip link show\nLink statistics : ip -s link show\n\nFiles\n\n/etc/hosts : There are number of names that resolve ip address\n/etc/services : List of commonly used services\n/etc/resolv.conf : Wejre DNS service are defined\n\n\n\n3.12.2 Validating Network Configuration\nCommands to review IP config\nip a\n\nTools\n\nnmcli : command line tool to manage network\nnmtui : graphical tool to manager network\nip : to list ip config\nnetstat : check ports and process\ntracepath : work similar traceourte tracepath access.redhat.com\nss : information about process that are opening up listening\n\nsample : ss -plunt\nsample2 : ss -lt : What is listening, t for TCP\n\n\n\n\n3.12.3 Configure Networking from the Command Line\nTool: nmcli an interface to the Network Manager daemon that supoprt tab complition\n\nConfig on : /etc/sysconfig/network-scripts\nHigh level of how network is configured :\n\nnmcli\n\n#output\nShow in New Window\n/etc/issue: ASCII text\n\n\n/bin/ : ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=a6cb40078351e05121d46daa768e271846d5cc54, for GNU/Linux 3.2.0, stripped\nShow in New Window\n40652348 -rw-rw-r-- 2 bruno bruno 6556 Aug 12 21:50 LICENSE\nShow in New Window\nwlp4s0: connected to Auto ALMAX-BRUNO\n    \"Intel Wireless-AC 3168NGW\"\n    wifi (iwlwifi), F8:94:C2:74:DC:04, hw, mtu 1500\n    ip4 default, ip6 default\n    inet4 192.168.15.8/24\n    route4 0.0.0.0/0\n    route4 169.254.0.0/16\n    route4 192.168.15.0/24\n    inet6 fe80::19da:6be7:d851:7ea9/64\n    route6 fe80::/64\n    route6 ::/0\n\np2p-dev-wlp4s0: disconnected\n    \"p2p-dev-wlp4s0\"\n    wifi-p2p, hw\n\nenp3s0f1: unavailable\n    \"Realtek RTL8111/8168/8411\"\n    ethernet (r8169), 80:FA:5B:4D:16:6D, hw, mtu 1500\n\nlo: unmanaged\n    \"lo\"\n    loopback (unknown), 00:00:00:00:00:00, sw, mtu 65536\n\nDNS configuration:\n    servers: 192.168.15.1\n    interface: wlp4s0\n\n    servers: fe80::aec6:62ff:fefc:7110\n    interface: wlp4s0\n\nUse \"nmcli device show\" to get complete information about known devices and\n\"nmcli connection show\" to get an overview on active connection profiles.\n\nConsult nmcli(1) and nmcli-examples(7) manual pages for complete usage details.\n\nConnection show :\n\nnmcli connection show\n\n\n#output\nNAME                UUID                                  TYPE      DEVICE \nAuto ALMAX-BRUNO    72e81729-a7a7-473b-8577-013339a4420b  wifi      wlp4s0 \nWired connection 1  44945eb1-1b88-305b-84c0-c0c13d340a0a  ethernet  --     \n\nDetails about connections : nmcli connection show \"Auto ALMAX-BRUNO\"\nSample of modify profile ipv4.dns\n\n# modify the dns \nncli con mod &lt;connection name&gt; ipv4.dns &lt;New DNS Name&gt;\n\n# or append, in this case we will have two values of dns\nncli con mod &lt;connection name&gt; +ipv4.dns &lt;New DNS Name&gt;\n\n# grep\nnmcli con show &lt;name&gt; | grep dns\n\n# reinitialize the profile\nncli con up &lt;connection name&gt;\n\n# check /etc/resolv.conf\ncat /etc/resolv.conf\n\nStatus:\n\nnmcli dev status\n\n#output\nDEVICE          TYPE      STATE         CONNECTION       \nwlp4s0          wifi      connected     Auto ALMAX-BRUNO \np2p-dev-wlp4s0  wifi-p2p  disconnected  --               \nenp3s0f1        ethernet  unavailable   --               \nlo              loopback  unmanaged     -- \n\nAdd conn\n\nsample\nnmcli con add con-name &lt;name of connection&gt; type ethernet ifname &lt;name of connection&gt;\n\nUp and Disconnect\n\n# Activate connection interface\nnmcli con up &lt;connection_name&gt;\n\n# disconect interface\nnmcli dev dis &lt;device&gt;\n\nDelete , reload and modify\n\n# Delete\nnmcli con del &lt;device&gt;\n\n# reload\nnmcli con reload\n\n# modify\nnmcli con mod &lt;name&gt;\n\nList\n\n# show NetworkManager of all Net Interface\nnmcli dev status\n\n# show connections\nnmcli con show\n\n# show config\nnmcli con show &lt;name&gt;\n\n\n\n3.12.4 Editing Network Configuration Files\nFiles :\nWe can modify the config file and perform reload nmcli con reload and nmcli con up &lt;name&gt; or use the nmcli commands\n\nEvery Device have a config file into /etc/sysconfig/network-scripts/ifcfg-*, where there are the hardware address, ipaddr, gateway, domain, etc\n\nSample after update ifcf-* file\nnmcli con reload\nmcli con down \"static-ens3\"\nnmcli con up \"static-ens3\"\n\n\n3.12.5 Configuring Hot Names and Name Resolution\n\nCommand\n\nhostname\nhostnamectl\nhostnamectl status\n\n\nhostnamectl status\n\n\n#output\n   Static hostname: turing\n         Icon name: computer-laptop\n           Chassis: laptop\n        Machine ID: bbada5cfcc9a4a80ac8dc5ce6d9f53aa\n           Boot ID: cf3c63ed65874b238e2f825055abbf1f\n  Operating System: Linux Mint 20.2\n            Kernel: Linux 5.4.0-81-generic\n      Architecture: x86-64\n\nTo update hostname\n\nhostnamectl set-hostname &lt;new_name&gt;\n\n# check\ncat /etc/hostname\nName resolution are om /etc/hosts , the order of resolution are on /etc/nsswitch.conf\n\nAdding additional DNS search and IP Address for IPv4 similar we can perform for IPv6\n\nsample\n# add \nnmcli con mod &lt;connection name&gt; +ipv4.dns &lt;IP&gt; +ipv4.dns-search &lt;name&gt;\n\n# up the connection\nnmcli con up &lt;connection name&gt;\n\n# check\ncat /etc/resolv.conf"
  },
  {
    "objectID": "Sysadmin1.html#archiving-and-transferring-files",
    "href": "Sysadmin1.html#archiving-and-transferring-files",
    "title": "3  Server Administrator I - RH124",
    "section": "3.13 Archiving and transferring files",
    "text": "3.13 Archiving and transferring files\n\n3.13.1 TAR\n\nCreate tar file\n\ntar -cf etc.tar /etc\n\nTest\n\ntar -tf etc.tar\n\nExtract\n\ntar -xf etc.tar\n\n# Extract one file\ntar -xf etc.tar etc/hosts\ncompression and tar\n\nCreate tar file and compress with gzip2\n\ntar -czf etc-backup-$(date +%F).tar.gz /etc\n\nCreate a tar file and compress with bzip2\n\ntar -cjf etc-backup-$(date +%F).tar.bz /etc\n\nCreate a tar file and compress with Xzip\n\ntar -cJf etc-backup-$(date +%F).tar.xz /etc\n\n\n\n3.13.2 SCP\n\nSecurely transfer files between systems\n\n\nCopy dir and files from another system\n\ncopy xf dir from servera to current dir ‘.’ \nscp -r user@servera:/xf . \n\nCopy without password\n\n# create keygen\nssh-keygen -N ''\n\n# copy id\nssh-copy-id servera\n\n# run scp command\nscp -r user@servera:/xf . \n\n\n3.13.3 SFTP\n# connect\nsftp user@servera\n\n# list\nsftp&gt; ls\n\n# create dir\nmkdir backup\ncd backup\n\n# local change dir\nsftp&gt; lcd /etc\n\n# upload hosts file\nsftp&gt; put hosts\n\n\n\n3.13.4 Synchronizing Files Between System\n\nTransfer using rsync\n\nrsync -Par servera:/xf . \nIf we update the data into servera on xf dir next time we perform rsync it will perform incremental receiving and download only the difference"
  },
  {
    "objectID": "Sysadmin1.html#intalling-and-updating-software-packages",
    "href": "Sysadmin1.html#intalling-and-updating-software-packages",
    "title": "3  Server Administrator I - RH124",
    "section": "3.14 Intalling and updating software packages",
    "text": "3.14 Intalling and updating software packages\nTo have the Red Hat software update and donwload the bins from RedHat vendor we need a subscription, there are 4 basics elements :\n\nYou have to register the server with Red Hat or in fact a satellite server\nAfter that need to subscribe the server to entitle it to update\nEnable repositories\nGo to lifecycle manamgent to track and manage entitlements via portal or subscription asset manager tool\n\nCommands\n\nStatus : subscription-manager status\nRegister : subscription-manager register\nAttach to subscription : subscription-manager attach --auto\nEnable repos : subscription-manager repos --disable='*'--enable='repos_name'\n\n\n3.14.1 Package manager\n\nrpm : Red Hat Package Manager is a popular format for installing software\n\n\nrpm database : keep track of what software is installed and versions\n\nQuery the rpm database to list all packages installed\n\n\nrpm -qa\n\nQuery a particular package\n\nrpm -q &lt;package&gt;\n\n#or \n\nrpm -qi &lt;package&gt;\n\nList files associate with packages\n\nrpm -ql &lt;package&gt;\n\nShow the config files\n\nrpm -qc &lt;package&gt;\n\nShow de documentation\n\nrpm -qd &lt;package&gt;\n\nShow the script that going to be executed\n\nrpm -q -p --scripts &lt;package&gt;.rpm\n\nInstall Limitations :  Cannot manage dependency\n\n\nrpm -i &lt;package&gt;\n\nWhich package provide a particular file\n\nrpm -qf &lt;file&gt;\n\nDownload using yumdownloader\n\nyumdownloader &lt;package&gt;\n\nQuery on rpm file, listing a list of files provided by this package\n\nrpm -qpl &lt;file&gt;.rpm\n\nQuery the config files into the rpm file\n\nrpm -qpc file.rpm\n\nExtract all the files and dirs that are inside a RPM file using rpm2cio\n\nrpm2cpio &lt;file&gt;.rpm | cpio -duim\n\n\n3.14.2 Inslalling and Update Software Packages with Yum\n\nSearch or get info on current repos\n\nyum search &lt;pacakge&gt;\n\nyum info &lt;pacakge&gt;\n\nQuery RPM files without have to download using repoquery\n\nrepoquery -l &lt;package&gt;\n\nFind out what package provide a particular file\n\nyum provides &lt;file like /etc/fstab&gt;\n\nyum profides *bin/authconfig\n\nInstall yum resolve all dependencies\n\nsudo yum install &lt;package&gt;\n\n#or\n\nsudo yum localinstall &lt;package&gt;.rpm\n\nUpdate\n\nsudo yum update &lt;package&gt;\n\nRemove Tip : do not use -y to review what going to be removed\n\nsudo yum remove &lt;package&gt;\n\nCheck groups\n\nyum group list\n\nCheck packages associate with a group for example Development Tools group\n\nyum group info \"Development Tool\"\n\nInstall in a particular group\n\nyum group install \"Development Tool\n\nLog can be checked on /var/log/dnf.rpm.log and we can see the history\n\nyum history\n\nWe can undo a transaction on history\n\nsudo yum history undo &lt;number_of_line&gt;\nTip\n\nList if the package are installed\n\nsudo yum list httpd mod_ssl\n\nInstall\n\n^list^-y install\n\n\n3.14.3 Enabling Yum Software Repositories\n\nList the repositories that we are connected and repo definition path are on /etc/yum.repos.d/\n\nyum repolist all\n\nEnable and disable repos using yum-config-manager\n\nyum-config-manager --disable &lt;repo_name&gt;\n\nyum-config-manager --enable &lt;repo_name&gt;\n\nList subscription manager repos list\n\nsubscription-manager-repos --list\n\n\n3.14.4 Managing Packages Module Streams\n\nModule is a set of RPM packages\n\n\n\n\n\n\n\nList modules and get info\n\nyum module list &lt;package/module like perl&gt;\n\n\nyum module info &lt;package/module like perl&gt;\n\n\n# List installed modules\nyum module list --installed\n\nInstall module stream\n\nyum module install &lt;package/module like perl&gt;\n\nRemove stream\n\nyum module remove &lt;package/module like perl&gt;\n\nDisable stream\n\nyum module disable perl\n\nTo add we can use the structure\n\n[BASE]\nname=Base\nbaseurl=httpd://......\ngpgcheck=0\n\nAlso if we have config-manager\n\nyum config-manager --add-repo &lt;URL&gt;"
  },
  {
    "objectID": "Sysadmin1.html#accessing-linux-file-systems",
    "href": "Sysadmin1.html#accessing-linux-file-systems",
    "title": "3  Server Administrator I - RH124",
    "section": "3.15 Accessing Linux File Systems",
    "text": "3.15 Accessing Linux File Systems\n\ndf -h : overview of various file systems\nblkid /dev/vda1 : show the UUID (universally-unique identifier) of file system\nfindmnt : tree overview of file system starting at roots\nlsblk : show overview of the various block devices like /dev/vda or /dev/sda\nlsblk -fp /dev/vdb check the UUID\n\nlsblk\n\n#Output\n\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0 931,5G  0 disk \n├─sda1   8:1    0   512M  0 part /boot/efi\n├─sda2   8:2    0 915,1G  0 part /run/timeshift/backup\n└─sda3   8:3    0  15,9G  0 part [SWAP]\n\nCheck the entire disk\n\n$ls -l /dev/sda\nbrw-rw---- 1 root disk 8, 0 Aug 22 08:50 /dev/sda\n[SWAP]\n\nb : Indicate that this is a block based device\ndu -sh : check space that has being occupied by a directory\n\ns : summary\nh : human read format\n\n\n$du -sh /var/log\n1,3G    /var/log\n\n\n3.15.1 Mount and Unmounting File systems\nTo Mount\n\nCheck the UUID of block device using blkid /dev/vb1 for instance\nCreate directory : mkdir -p &lt;path&gt;\nMount : mount UUID=\"XXX\" /&lt;path&gt;\nCheck the return code echo $? (0 means successfully)\nCheck with df -h or mount command\n\nTo Unmounting\n\numount &lt;/dev/vdb1&gt;\n\n\n\n3.15.2 Locating Files on the System\nfind\nBasic find syntax :\n\nfind &lt;where&gt; &lt;how&gt; &lt;what is lokking for&gt;\n\nSome examples :\n\nfind / -name ssh_config : looking for particular name\nfind / -iname ssh_config: using i going to use case insensitive search\nfind /usr -iname \"*.pdf\" : looking all files that end with pdf\nfind / -user &lt;user_name&gt; : find files for a particular user\nfind / -user &lt;user_name&gt; -delete : find and delete\nfind / -type f -user rick -size 10M : find files from user rick with 10MB (+10M more than 10MB , -10M less than 10MB)\nfind /home -size +10M -exec ls -lh {} \\; : find on home files more than 10MB and list with ls -lh\nfind /home -size +10M iname \"*.mkv\" -exec rm -f {} \\; : find files on home with more than 10MB ending with .mkv and execute rm -f on each file, similar find /home -size +10M iname “*.mkv” -delete`\nfind /home -type f -perm /111 : find files on home that have execute permission (111)\nfind /home -min -60 : find files modified in last 60min\nfind / -user &lt;user_name&gt; -exec -cp {} /&lt;path_copy&gt;/ \\;\n\nlocate\npre-req : run the command updatedb to create an index of the files on file system\n\nlocate &lt;file&gt; : basic locate a file"
  },
  {
    "objectID": "Sysadmin1.html#analysing-server-and-getting-support",
    "href": "Sysadmin1.html#analysing-server-and-getting-support",
    "title": "3  Server Administrator I - RH124",
    "section": "3.16 Analysing Server and Getting Support",
    "text": "3.16 Analysing Server and Getting Support\ncockpit service used to analysing and managing remote servers, this is a web user interface that use TCP 9090\n\nEnable : systemctl enable --now cokpit.socket\nReload firewall daemon and set permanent : firewall-cm --add-service cockpit --permanent and firewall-cmd --reload\nlink : https://:9090/system\n\n\n3.16.1 Deteting and Resolving issues with Red Hat Insights\n\nIt is a form of AI which is available as a software as a service\n\nInstall Insights\n\nRegister and attach the server on subscription-manager\n\nsudo subscription-manager register --auto-attach\n\nInstall client Insights and register\n\nsudo yum install -y insights-client \n\nsudo insights-client --register\n\n\nGo to insights dashboard cloud.readhat.com/insight"
  },
  {
    "objectID": "Sysadmin1.html#extra",
    "href": "Sysadmin1.html#extra",
    "title": "3  Server Administrator I - RH124",
    "section": "3.17 Extra",
    "text": "3.17 Extra\n\n3.17.1 Create 8G file disk\n\nUse dd command to create 8G disk file\n\ndd if=/dev/zero of=&lt;/path/diskfile.img&gt; bs=1M count=8192\n\ncreate loop device\n\nloseup -fP diskfile.img\n\nlist the device\n\nlosetup -a\n4.Can use new diskfile as a device\nfdisk /dev/loop0\n\n\n3.17.2 Find users wit Set UID\nfind / -perm /4000 -exec ls -l {} \\ 2&gt; /dev/null\n\n\n3.17.3 Show dump device\nxxd -l 512 /dev/sda | less\n\n\n3.17.4 Configure labels of FS\ntune2fs -L &lt;label&gt; /dev/sda&lt;number&gt;"
  },
  {
    "objectID": "OpenShift2.html#describing-the-red-hat-openshift-container-plataform",
    "href": "OpenShift2.html#describing-the-red-hat-openshift-container-plataform",
    "title": "7  Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280",
    "section": "7.1 Describing the Red Hat OpenShift Container Plataform",
    "text": "7.1 Describing the Red Hat OpenShift Container Plataform\n\n7.1.1 Describing OpenShift Container Platform\nRHOCP is based on Kubernetes and allow manage container at scale, a container orchestrator platform manages a cluster service that runs multiple containerized applications .\nSolutions :\n\nRed Hat OpenShift Container Platform : Enterprise-ready Kubernetes environment for building, deploying and managing container-based applications on any public or private data center. Red Hat decide when update to newer releases and which addition component to enable\nRed Hat OpenShift Dedicated : Managed OpenShift environment in a public cloud, AWS, GCP, Azure, or IBM Cloud, all features of RHOCP, however Red Hat manage the cluster, we have some control of decisions as when to update to a newer release or to install add-ons.\nRed Hat OpenShift Online : Public container platform shared across multiple customers, Red Hat manages the cluster life cycle.\nRed Hat OpenShift Kubernetes Engine : Subset of the features present in Red Hat OpenShift RCOP, such as coreOS, CRI-O engine, web console, etc\nRed Hat Code Ready Container : Minimal installation of OpenShift that we can run on a laptop to development and experimentation.\n\nBelow the services and features of Openshift\n\n\n\n\n\n\nIntroduction of OpenShift features\nComparing OpenShift Container Platform vs OpenShift Kubernetes Engine:\n\n\n\n\n\nFeatures :\n\nHigh Availability : etc cluster store the state of the OpenShift Cluster and Applications\nLightweight OS : CoreOS focuses on agility, portability and security\nLoad Balancing : External via API, HAProxy load balance for external app and internal load balance\nAutomating Scaling : can adapt to increased application traffic in real time by automatically starting new containers and terminate when the load decrease.\nLogging and Monitoring : Advanced monitoring solution based on Prometheus, also advanced logging solution based on ElasticSearch.\nService Discovery : Internal DNS, application can rely on friendly names to find other app and services\nStorage : Allow automatic provisioning of storage on popular cloud providers and visualization platforms\nApplication Management : Automate the development and deploy, automatic build containers based on source code using Source-To-Image (S2I) solution.\nCluster Extensibility : Rely on standard extension from kubernetes, Openshift packages these extensions as operators for ease of installation, update, and management. Also include Operator Lifecycle Manager (OLM), which facilitates the discovery, installation, and update of applications and infrastructure components packaged as operators\n\nOpenShift also includes the Operator Lifecycle Manager (OLM) which facilitates the discovery, installation, and update of applications and infrastructure components packaged as operators\n\n\n7.1.2 Architecture of OpenShift\nOpenShift architecture is based on declarative the nature of kubernetes. In a declarative architecture, you change the state of the system and the system updates itself to comply with the new state.\n\nKubernetes cluster consists of a set of nodes that run the kubelet system service and a container engine.\nOpenShift runs exclusively the CRI-O container engine. Some nodes are control plane nodes that run the REST API, the etcd database, and the platform controllers\n\nOpenShift is a Kubernetes distribution that provides many of these components already integrated and configured, and managed by operators. OpenShift also provides preinstalled applications, such as a container image registry and a web console, managed by operators.\n\n\n7.1.3 Cluster Operators\n\nKubernetes operators are applications that invoke the Kubernetes API to manage Kubernetes resources.\n\n\nCustom resources (CR) : store settings and configurations\nCustom resource definition (CRD) : the syntax of a custom resource is defined by a custom resource definition\n\nMost operators manage another application; for example, an operator that manages a database server.\nThe purpose of an operator is usually to automate tasks.\n\nOperator Framework\n\nOperator Software Development Kit (Operator SDK) : Golang library and source code. Also provide container image and ansible playbook examples.\nOperator Life Cycle Manager (OLM) : Application that manages the deployment, resource utilization, updates and deletion of operators. The OLM itself is an operator that comes preinstalled with OpenShift.\n\n\nOperatorHub\nOperatorHub provides a web interface to discover and publish operators that follow the Operator Framework standards.\n\nRed Hat Marketplace is a platform that allow access a curated set of enterprise operators that can be deployed on OpenShift or a kubernetes cluster\nOpenShift Cluster Operators : regular operators except that they are not managed by the OLM, they are managed by OpenShift Cluster Version Operators, also called as first level operator."
  },
  {
    "objectID": "OpenShift2.html#verifying-the-health-of-a-cluster",
    "href": "OpenShift2.html#verifying-the-health-of-a-cluster",
    "title": "7  Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280",
    "section": "7.2 Verifying the Health of a Cluster",
    "text": "7.2 Verifying the Health of a Cluster\n\n7.2.1 Intro to OpenShift Installtion Methods\nFull-stack Automation : Installe provisions all compute, storage and network, on cloud or virtualization\nPre-existing Infrastructure : we can configure a set of compute, storage and network resources, can be configured on bare-metal, cloud or virtualizations providers\nDeploy process\nInstall stages that results in a fully running OpenShift control plane :\n\nThe bootstrap machine boots and starts hosting the remote resources for booting the control plane machine, “like a repo”\nControl plane machine fetch the remote resources from bootstrap machine\nControl plane form an Etcd cluster\nBootstrap machine starts a temp kubernetes control plane\nThe temp control plane schedule the control plane to the control plane machines\nThe temp control plane shuts down\nBootstraps injects components to OpenShift into control plane\nInstaller tears down the bootstrap machine\n\nWe can customize the installer by adding custom storage class, change custom resources, adding new operators and defining new machine sets.\n\n\n7.2.2 Troubleshooting OpenShit Cluster and Applications\nCommands :\n\noc get nodes : Status of each node\noc adm top nodes : CPU and Memory of each node\noc describe node &lt;my_node-name&gt; : Resources available and used\npc get clusterversion : version of cluster\noc describe clusterversion : mode details about cluster status\noc get clusteroperators : list of all cluster operators\noc adm node-logs -u &lt;unit&gt; &lt;my-node-name&gt; : view logs\n\nUnit can be : crio, kubelet, etc\n\noc adm node-logs &gt;my-node-name&gt; : display all journal logs of a node\noc logs &lt;my-pode-name&gt; show de logs of pod\noc logs &lt;my-pod-name&gt; -c &lt;my-container-name&gt; : show logs of container\nDebug\n\noc debug node/&lt;my-node-name&gt;\nchroot /host\nsystemctl is-active kubelet\n\n\noc debug node/&lt;my-node-name&gt;\nchroot /host\ncrictl ps\n\n\n\nDebug as root\n\n[user@host ~]$ oc debug deployment/my-deployment-name --as-root\n\nChanging a running container\n\noc rsh &lt;my-pod-name&gt; open shell inside the a pod\noc cp /local/path my-pod-name:/conatiner/path : copy files\noc port-forward my-pod-name local-port:remote-port : create a tcp tunel\n\noc get pod --level 6 : Show logs on different levels\noc whoami -t : Make a token that the oc command use\n\n\n\n7.2.3 Introducing OpenShift Dynamic Storage\nContainer offers two main ways of maitaining persistent storage, using volumes and bind mounts.\n\nVolumes are managed manuall by admin or dynamically via storage class\nDevs can mount a local directory into a container using bind mount\n\nOpenShift use Kubernetes persistent volume framework to manage persistent storage dynamic or static.\nA persistent volume claim (PVC), where appl going to request a type of storage, belongs to a specific project. To create a PVC, you must specify the access mode and size, among other options. Once created, a PVC cannot be shared between projects. Developers use a PVC to access a persistent volume (PV).\n\nVerify the Dynamic Provisioned storage\n\n[user@host ~]$ oc get storageclass\n\nDeploying Dynamically Provisioned Storage, to add volume to an application create a PersistentVolumeClaim resource and add it to application as a volume\n\n[user@host ~]$ oc set volumes deployment/example-application \\\n--add --name example-storage --type pvc --claim-class nfs-storage \\\n--claim-mode rwo --claim-size 15Gi --mount-path /var/lib/example-app \\\n--claim-name example-storage\n\nDeleting Persistent Volume Claims\n\n[user@host ~]$ oc delete pvc/example-pvc-storage"
  },
  {
    "objectID": "OpenShift2.html#configuring-authentication-and-authorization",
    "href": "OpenShift2.html#configuring-authentication-and-authorization",
    "title": "7  Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280",
    "section": "7.3 Configuring Authentication and Authorization",
    "text": "7.3 Configuring Authentication and Authorization\n\n7.3.1 Identity\n\nUser : Interact with the API server\nIdentity : keeps a record of successful authentication attempts from a specific user and identity provider. Only a single user resource is associated with an identity resource.\nService Account : enable you to control API access without the need to borrow a regular user’s credentials\nGroup : set of users\nRole : defines a set of permissions that enables a user to perform API operations over one or more resources\n\n\nThe authentication and autorization security layers\nThe authentication layer authenticates the userusing (OAuth Access tokens or X.509 Client Certification). Upon successful authentication, the authorization layer decides to either honor or reject the API request. The authorization layer use RBAC policies.\n\nAuthentication Operator : OpenShift OAuth server can be configured to use many identity providers.\n\nHTPasswd : Validate user and password against a secret using htpasswd\nKeystone : Enables shared authentication with an OpenStack Keystone v3 server.\nLDAP : Configure LDAP`identity provider to validate against an LDAPv3 server\nGitHub or GitHub Enterprise : use OAuth github authentication\nOpenID Connect : Use Code Flow integrates with an OpenID connect\n\nDuring install OpenShift create a kubeconfig file that contains specific details and parameter to be used by CLI\n\nINFO Run 'export KUBECONFIG=root/auth/kubeconfig' to manage the cluster with 'oc'.\n\nTo use kubeconfig file\n\n[user@host ~]$ export KUBECONFIG=/home/user/auth/kubeconfig\n[user@host ~]$ oc get nodes\n\n# or\n\n[user@host ~]$ oc --kubeconfig /home/user/auth/kubeconfig get nodes\n\n\n\n7.3.2 Configuring the HTPasswd Identity Provider\nNeed to update the OAuth custom resource add the .spec.identityProviders array :\napiVersion: config.openshift.io/v1\nkind: OAuth\nmetadata:\n  name: cluster\nspec:\n  identityProviders:\n  - name: my_htpasswd_provider\n    mappingMethod: claim\n    type: HTPasswd\n    htpasswd:\n      fileData:\n        name: htpasswd-secret\n\nTo export the current OAuth cluster resource in YAML format\n\n[user@host ~]$ oc get oauth cluster -o yaml &gt; oauth.yaml\n\nOpen the file edit and replace on cluster\n\n[user@host ~]$ oc replace -f oauth.yaml\n\nManaging Users with the HTPasswd Identity Provider\n\nCreate an HTPasswd File\n\n[user@host ~]$ htpasswd -c -B -b /tmp/htpasswd student redhat123\n\nAdd or update credentials\n\n[user@host ~]$ htpasswd -b /tmp/htpasswd student redhat1234\n\nCreate the HTPasswd Secret\n\n[user@host ~]$ oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config\n\nExtracting Secret Data\n\noc extract secret/htpasswd-secret -n openshift-config --to /tmp/ --confirm /tmp/htpasswd\n\nUpdating the HTPasswd Secret\n\noc set data secret/htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config\n\nWatch the redeploy pods\n\n[user@host ~]$ watch oc get pods -n openshift-authentication\n\nDeleting Users and Identities\n\nTo delete the user from htpasswd\n\n[user@host ~]$ htpasswd -D /tmp/htpasswd manager\n\nUpdate the secret to remove all remnants of the user\n\noc set data secret/htpasswd-secret  --from-file htpasswd=/tmp/htpasswd -n openshift-config\n\nTo remove the user resource\n\n[user@host ~]$ oc delete user manager\n\nTo delete identity resource\n\n# check\n[user@host ~]$ oc get identities | grep manager\n\n# to delete\n[user@host ~]$ oc delete identity my_htpasswd_provider:manager\n\nAssigning Administrative Privileges\n# assigns the cluster-admin role to the student user.\n[user@host ~]$ oc adm policy add-cluster-role-to-user cluster-admin student"
  },
  {
    "objectID": "OpenShift2.html#configuring-application-security",
    "href": "OpenShift2.html#configuring-application-security",
    "title": "7  Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280",
    "section": "7.4 Configuring Application Security",
    "text": "7.4 Configuring Application Security\n\n7.4.1 Managing Sensitive Information with secrets\nKubernetes and OpenShift use secret resources to hold sensitive information :\n\nPassword\nSensitive configuration files\nCredentials to an external resource, such as SSH Key or OAuth token\n\nSecrete is Base64-encoded, not stored in plain text, we can encrupt the Etcd and that encrypt secrets, config maps, routes, OAuth.\n\nFeatuers of Secrets\n\nCan be shared within project namespace\nAdministrators can create and manage secrets that other team cdan reference in thei deploy config\nSecret data is injected into pods\nAfter a secret value changes we must create a new pods to inject the new data\nOpenShift exposes sensitive data to a pod as environment variable\n\n\nUse cases for secrets\n\nCredentials\n\nIf an application expects to read sensitive information from a file, then you mount the secret as a data volume to the pod.\nSome applications use environment variables to read configuration and sensitive data. You can link secret variables to pod environment variables in a deployment configuration.\n\nTLS and Key Pairs Used to to secure communication to a pod\n\nDevelopers can mount the secret as a volume and create a pass through route to the application.\n\n\nCreating Secret\n\nUsing Key-value\n\n[user@host ~]$ oc create secret generic secret_name --from-literal key1=secret1 --from-literal key2=secret2\n\nUsing Key names\n\n[user@host ~]$ oc create secret generic ssh-keys --from-file id_rsa=/path-to/id_rsa --from-file id_rsa.pub=/path-to/id_rsa.pub\n\nCreate a TLS secret\n\n[user@host ~]$ oc create secret tls secret-tls --cert /path-to-certificate --key /path-to-key\nExposing Secrets to Pods\n\nCreate the secret\n\n[user@host ~]$ oc create secret generic demo-secret --from-literal user=demo-user --from-literal root_password=zT1KTgk\n\nModify the env variable section of the deploy config\n\nenv:\n  - name: MYSQL_ROOT_PASSWORD\n    valueFrom:\n      secretKeyRef:\n          name: demo-secret\n          key: root_password\n\nSet application environment variables from either secrets or configuration maps.\n\n[user@host ~]$ oc set env deployment/demo --from secret/demo-secret --prefix MYSQL_\n\nA secret can be mounted\n\n[user@host ~]$ oc set volume deployment/demo --add --type secret --secret-name demo-secret --mount-path /app-secrets\n\nConfiguration Map\nSimilar to secrets, configuration maps decouple configuration information from container images. Unlike secrets, the information contained in configuration maps does not require protection\n[user@host ~]$ oc create configmap my-config --from-literal key1=config1 --from-literal key2=config2\n\nUpdating Secrets and Configuration Maps\n\nExtract the last data\n\n[user@host ~]$ oc extract secret/htpasswd-ppklq -n openshift-config --to /tmp/ --confirm\n\nUpdate and save the files\nUse oc set data to update\n\n[user@host ~]$ oc set data secret/htpasswd-ppklq -n openshift-config --from-file /tmp/htpasswd\n\n\n7.4.2 Controlling Application Permissions with Security Contect Contrainst\nSecurity Context Constraints (SCCs) : a security mechanism that restricts access to resources, but not to operations in OpenShift.\n\nCheck the list of SCCs\n\noc get scc\n\nGet additional info about an SCC\n\n[user@host ~]$ oc describe scc anyuid\n\nView the security contect constraint that pod uses\n\n[user@host ~]$ oc describe pod console-5df4fcbb47-67c52 -n openshift-console | grep scc\n\nUse the scc-subject-review subcommand to list all the security context constraints that can overcome the limitations of a container:\n\n[user@host ~]$ oc get pod podname -o yaml | oc adm policy scc-subject-review -f -\n\nTo change the container to run using a different SCC, you must create a service account bound to a pod.\n\n[user@host ~]$ oc create serviceaccount service-account-name\n\nAssociate the service account with an SCC\n\n[user@host ~]$ oc adm policy add-scc-to-user SCC -z service-account\n\nModify an existing deployment or deploy config\n\n[user@host ~]$ oc set serviceaccount deployment/deployment-name service-account-name"
  },
  {
    "objectID": "OpenShift2.html#configuring-openshift-networking-for-applications",
    "href": "OpenShift2.html#configuring-openshift-networking-for-applications",
    "title": "7  Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280",
    "section": "7.5 Configuring OpenShift Networking For Applications",
    "text": "7.5 Configuring OpenShift Networking For Applications\n\n7.5.1 OpenShift Software-defined Networking\n\nSDN (software-defined network) is a networking model that allows you to manage network services through the abstraction of several networking layers.\n\nSDN use CNI plug-ins that allows containers inside pods share network resources\nCommon CNI plug-ins : OpenShift SDN, OVN-Kubernetes and kuryr\n\nOpenShift implmenets the SDN to manage networking infrastructure of the cluster and user applications\n\nWe can :\n\nManage network traffic and decice how to expose the applications\nManage communications between containers that run in the same projects\nManage communication between pods\nManage network from pod to a service\nManage network from an external network to a service, or from containers to external networks\n\n\nUsing Services for accessing pods\nServices rely on selectors (labels) that indicate which pods receive the traffic through the service. Each pod matching these selectors is added to the service resource as an endpoint. As pods are created and killed, the service automatically updates the endpoints.\nOpenShift uses two subnets (onde for pods and another one for services). The traffic is forwarded in a transparent way to the pods; an agent manages routing rules to route traffic to the pods that match the selectors.\n\nDNS Operator\n\nThe DNS operator deploys and runs a DNS server managed by CoreDNS.\nThe DNS operator provides DNS name resolution between pods\nTo check\n\n[user@demo ~]$ oc describe dns.operator/default\n\nCluster Network Operator\nOpenShift Container Platform uses the Cluster Network Operator for managing the SDN\n\nTo consult the SDN configuration which is managed by the Network.config.openshift.io\n\n[user@demo ~]$ oc get network/cluster -o yaml\n\n\n7.5.2 Exposing Applications for External Access\nAccessing Application from External Networks\n\nCan use :\n\nHTTP\nHTTPS\nTCP\nnon-TCP\nOthers using Ingress and Route\n\n\nOpenShift Route allow expose applicationn to external network\n\nDescribing Methods for Managing Ingress Traffic\n\nOpenShift implements the Ingress Controller with a shared router service that runs as a pod inside the cluster.\n\n\nRoute :\n\nProvide ingress traffic to services in the cluster\nProvide features that may not be supported by Kubernetes ingress such as TLS re-encryption, TLS passthrough and split traffic for blue-green deployments\n\n\nIngress :\n\nA Kubernetes resource that provides some of the same features as routes\nAccept external requests and proxy them based on the route.\nOnly allow HTTP, HTTPS and server name identification SNI and TLS with SNI\n\n\nOthers\nExternal load balancer : A load balancer instructs OpenShift to interact with the cloud provider in which the cluster is running to provision a load balancer.\nService external IP : This method instructs OpenShift to set NAT rules to redirect traffic from one of the cluster IPs to the container\nNodePort : With this method, OpenShift exposes a service on a static port on the node IP address. You must ensure that the external IP addresses are properly routed to the nodes.\n\nCreating Routes\n[user@host ~]$ oc expose service api-frontend --hostname api.apps.acme.com\n\nSecuring Routes\n\nEdge\nPassthrough\nRe-encryption\nhow to create a secure edge route with TLS certificate\n\n[user@host ~]$ oc create route edge --service api-frontend --hostname api.apps.acme.com --key api.key --cert api.crt\n\n\n7.5.3 Configuring Network Policies\n\nNetwork policies allow you to configure isolation policies for individual pods\n\nTo manage network communication between two namespaces, assign a label to the namespace that needs access to another namespace. The following command assigns the name=network-1 label to the network-1 namespace:\n[user@host ~]$ oc label namespace network-1 name=network-1"
  },
  {
    "objectID": "Sysadmin3.html#intro-to-ansible",
    "href": "Sysadmin3.html#intro-to-ansible",
    "title": "5  Server Administrator III - RH294",
    "section": "5.1 Intro to Ansible",
    "text": "5.1 Intro to Ansible\nAnsible is an open source automation platform. It is a simple automation language that can perfectly describe an IT application infrastructure in Ansible Playbooks. It is also an automation engine that runs Ansible Playbooks.\n\n5.1.1 Ansible Concepts and Architecture\n\nControl nodes : Where Ansible is installed and runs and has copies of Ansible project files, also can be an Administrator server, where Tower will run.\nManaged hosts : list of servers organized in inventory list\nInventory :\n\nStatic : List of servers\nDynamic : Program that connect to provider and search for list of machines\n\nPlaybook : List of tasks that going to be converted in python script to run in each host, those tasks are expressed in YAML format in a text file\n\n\n\n\n\n\n\n\n5.1.2 Install Ansbile\n\nTo install ansible\n\nyum install ansbile\n\nTo check version\n\nansbile --version\n\nRHEL8 can use the plataform python package\n\nyum list installed plataform-python\n\nTo register on RedHat and Enable repository\n\nsubscription-manager register\n\nubscription-manager repos --enable ansible-2-for-rhel-8-x86_64-rpms\n\nTo install python36\n\nyum module install python36\n\nTo list the modules\n\nansible-doc -l\n\n\n5.1.3 Implementing an Ansible Playbook"
  },
  {
    "objectID": "Sysadmin3.html#deploying-anisble-and-implementing-playbooks",
    "href": "Sysadmin3.html#deploying-anisble-and-implementing-playbooks",
    "title": "5  Server Administrator III - RH294",
    "section": "5.2 Deploying Anisble and Implementing Playbooks",
    "text": "5.2 Deploying Anisble and Implementing Playbooks\n\n5.2.1 Building an Ansible Invetory\nStatic inventory file is a text file that specifies the managed hosts that Ansilbe targets, it is located on /etc/ansible/hosts as default\nSample of YAML file\nweb1.example.com\nweb2.example.com\ndb1.example.com\ndb2.example.com\n192.0.2.42\nWe can also organize the inventory in groups using [], ,hosts can be in multiple groups\n[webservers]\nweb1.example.com\nweb2.example.com\n192.0.2.42\n\n[db-servers]\ndb1.example.com\ndb2.example.com\nWe also can configure nested groups with :children sufix\n[usa]\nwashington1.example.com\nwashington2.example.com\n\n[canada]\nontario01.example.com\nontario02.example.com\n\n[north-america:children]\ncanada\nusa\nThe hosts can also be specified with Ranges [START:END]\n[usa]\nwashington[1:2].example.com\n\n[canada]\nontario[01:02].example.com\nTo verify the inventory we can use the commands below\n# his command verify if machine is present in inventory\nansible washington1.example.com --list-hosts\n\n\n# List all hosts in canada group\nansible canada --list-hosts\n\nTo list from an specific inventory file, -i makes ansible use your inventory file in the current working directory instead of the system /etc/ansible/hosts inventory file\nansible all -i inventory --list-hosts\n\nList ungrouped hosts\nansible ungrouped -i inventory --list-hosts\nList hosts from specific group called us\nansible us -i inventory --list-hosts\nList inventory as a graph\nansible-invetory --graph -i /etc/anisble/hosts\n\n\n5.2.2 Managing Ansible Configuration Files\nThe ansible configuration file is located at /etc/ansible/ansible.cfg as default, but ansible looks at ~/.ansible.cfg that overight the default, however if the ./ansible.cfg exists in the directory in which the ansible command is executed, it is used instead of the global file or the user personal file. We can also configure the environment variable ANSIBLE_CONFIG to set the ansible.cfg , in this case all commands going to point to this config file.\nTo list the config file\nansible --version\n\nansible servers --list-hosts -v\n\nTo list the ansible config\nanisble config\nSample of config file\n[defaults]\ninventory = ./inventory\nremote_user = user\nask_pass = fals\n\n[privilege_escalation]\nbecome = true\nbecome_method = sudo\nbecome_user = root\nbecome_ask_pass = false\n\ninventory : Specifies the path to the inventory file.\nremote_user : The name of the user to log in as on the managed hosts. If not specified, the current user’s name is used.\nask_pass : Whether or not to prompt for an SSH password. Can be false if using SSH public key authentication.\nbecome : Whether to automatically switch user on the managed host (typically to root) after connecting. This can also be specified by a play.\nbecome_method : How to switch user (typically sudo, which is the default,but su is an option).\nbecome_user : The user to switch to on the managed host (typically root, which is the default).\nbecome_ask_pass : Whether to prompt for a password for your become_method. Defaults to false.\n\nTo list all the config options we can read the /etc/ansible/ansible.cfg or run the command below to dump\nansible-config dump\nSample of ansible playbook to deploy a public key\n- name: Public key is deployed to managed hosts for Ansible\n  hosts: all\n\ntasks:\n- name: Ensure key is in root's ~/.ssh/authorized_hosts\n    authorized_key:\n      user: root\n      state: present\n      key: '{{ item }}'\n    with_file:\n      - ~/.ssh/id_rsa.pub\n\n\n5.2.3 Running Ad Hoc Commands\n\nSample date command to a host\n\nansible servera.lab.example.com -m command -a date\n\nList the modules\n\nansile-doc -l\n\nCheck documentation for a module\n\nansible-doc &lt;module_name&gt;\n\nUsing the module user to create and remove a user\n\n# create\nansible &lt;server&gt; -m user -a name=&lt;name_of_user&gt;\n\n# remove\nansible &lt;server&gt; -m user -a \"name=&lt;name_of_user&gt; state=absent\"\n\nSpecfing user and become to copy as root\n\nansible all -m copy -a 'content=\"Managed by Ansible\\n\" dest=/etc/motd' -u devops --become\n\n\n5.2.4 Writing and Running Playbooks\n\nUsing command\n\n[student@workstation ~]$ ansible -m user -a \"name=newbie uid=4000 state=present\" servera.lab.example.com\n\nUsing playbook\n\n---\n- name: Configure important user consistently\n  hosts: servera.lab.example.com\n  tasks:\n    - name: newbie exists with UID 4000\n      user:\n        name: newbie\n        uid: 4000\n        state: present\n\nTip for configure vim as editor\n\nvim ~/.vimrc\n\n# add\nautocmd FileType yaml setlocal ai ts=2 sw=2 et nu cuc\nautocmd FileType yaml colo desert\n\n\n\n5.2.5 Running Playbooks\nSimple Command\nansible-playbook site.yml\nWe can increase the verbosity of output using -v, -vv , -vvv or -vvvv and also **check the syntax* like:\nansible-playbook --syntax-check xxx.yml\nAnother option is execute as a Dry Run using option -C\nansible-playbook -C xxx.yml\n\n\n\n5.2.6 Implementing Muliple Plays\nA playbook is a YAML file containing a list of one or more plays, if a playbook contains multiple plays, each play may apply its tasks to a separate set of hosts.\nSample\n---\n# This is a simple playbook with two plays\n- name: first play\n  hosts: web.example.com\n  tasks:\n    - name: first task\n      yum:\n        name: httpd\n        status: present\n\n    - name: second task\n      service:\n        name: httpd\n        enabled: true\n\n- name: second play\n  hosts: database.example.com\n  tasks:\n    - name: first task\n      service:\n        name: mariadb\n        enabled: true\nPrivilege Escalation\nThose configuration can be set on ansible.cfg configuration file or at task level\n\nbecome : True or False to enable or disable escalation\nbecome_method : sudo/pbrun method of escalation\nbecome_user : privilege user\nremote_user : User that runs the tasks\n\n\n\n5.2.7 Finding Modules for Task\nThe command ansible-doc -l will list all the modules on the current version\nansible-doc -l\nTo list detail about documentation, also to access the examples of playbooks go to ansible-doc moudule and run the /EXAMPLES\nansible-doc &lt;module&gt;\n\n# check examples\n/EXAMPLES"
  },
  {
    "objectID": "Sysadmin3.html#managing-variables-and-facts",
    "href": "Sysadmin3.html#managing-variables-and-facts",
    "title": "5  Server Administrator III - RH294",
    "section": "5.3 Managing Variables and Facts",
    "text": "5.3 Managing Variables and Facts\nWe can set a variable that affects a group of hosts or only individual hosts. Some variables are facts that can be set by Ansible based on the configuration of a system. Other variables can be set inside the playbook, and affect one play in that playbook, or only one task in that play.\nThere are also set extra variables on the ansible-playbook command line by using the –extra-vars or -e option and specifying those variables, and they override all other values for that variable name.\nSimple list of ways to define a variable, ordered from lowest precedence to highest :\n\nGroup variable in inventory\nGroup variable in files in a group_vars sub dir in the same dir as inventory or playbook\nHost variable in the inventory\nHost variables in files in a host_var sub dir in the same dir as the inventory or playbook\nHost facts, discoverd at runtime\nPlay variables in the playbook(vras and var_files)\nTask variables\nExtra variables on the command line\n\n\nA variable that is set to affect the all host group will be overridden by a variable that has the same name and is set to affect a single host.\n\n\n5.3.1 Variables in playbook\n- hosts: all\n  vars:\n    user: joe\n    home: /home/joe\n\nUsing external files in the vars_files directive may be used\n\n- hosts: all\n  vars_files:\n    - vars/users.yml\n\nUsing variables {{ var_name }}, using quotes is mandatory if the variable is the first element to start a value\n\nvars:\n  user: joe\n\ntasks:\n  # This line will read: Creates the user joe\n  - name: Creates the user {{ user }}\n    user:\n      # This line will create the user named Joe\n      name: \"{{ user }}\"\n\n\n5.3.2 Host and group variables\n\nDefining the ansible_user host variable for demo.example.com:\n\n[servers]\ndemo.example.com ansible_user=joe\n\nDefining the user group variable for the servers host group.\n\n[servers]\ndemo1.example.com\ndemo2.example.com\n\n[servers:vars]\nuser=joe\n\n\n5.3.3 Using directories to populate host and group variables\nThe recommended practice is to define inventory variables using host_vars and group_vars directories, and not to define them directly in the inventory files\n[admin@station project]$ cat ~/project/inventory\n[datacenter1]\ndemo1.example.com\ndemo2.example.com\n\n[datacenter2]\ndemo3.example.com\ndemo4.example.com\n\n[datacenters:children]\ndatacenter1\ndatacenter2\n\nVariable for the databaceters group\n\n[admin@station project]$ cat ~/project/group_vars/datacenters\npackage: httpd\n\nVariable for each datacenetr\n\n[admin@station project]$ cat ~/project/group_vars/datacenter1\npackage: httpd\n\n[admin@station project]$ cat ~/project/group_vars/datacenter2\npackage: apache\n\nVariable for each host\n\n[admin@station project]$ cat ~/project/host_vars/demo1.example.com\npackage: httpd\n\n[admin@station project]$ cat ~/project/host_vars/demo2.example.com\npackage: apache\n\n[admin@station project]$ cat ~/project/host_vars/demo3.example.com\npackage: mariadb-server\n\n[admin@station project]$ cat ~/project/host_vars/demo4.example.com\npackage: mysql-server\n\n\n5.3.4 Overrding variable from command line\n[user@demo ~]$ ansible-playbook main.yml -e \"package=apache\"\n\n\n5.3.5 Secrets\nAnsible Vault can be used to encrypt and decrypt any structured data file used by Ansible\n[student@demo ~]$ ansible-vault create secret.yml\nNew Vault password: redhat\nConfirm New Vault password: redhat\nWe can use view to view the content, encrypt and decrypt option .\nTo run a playbook with vault\n[student@demo ~]$ ansible-playbook --vault-id @prompt site.yml\nVault password (default): redhat\n\n# or \n[student@demo ~]$ ansible-playbook --vault-password-file=vault-pw-file site.yml\n\n\n\n5.3.6 Managing Facts\nAnsible facts are variables that are automatically discovered by Ansible on a managed host, every play runs the setup module automatically before teh first task to gather facts, this is report on Gathering Facts task, for example ?\n\nhostname\nkernel version\nnetwork interface\nIP\nOS info, CPUs, disk, memory, etc\n\nTo turn off the facts we can set the option gather_facts: no and the facts will not be collected.\nTo create custom facts we need to speficy on /etc/ansible/facts.d/.fact the name need to end with .fact below one example\n[packages]\nweb_package = httpd\ndb_package = mariadb-server\n\n[users]\nuser1 = joe\nuser2 = jane\n\n\n5.3.7 Magic Variables\nThose variables are not facts or configured on setup but are also automatically set by Ansible\n\nhostvars : Contains the variables for managed hosts\ngroup_names : Lists all groups the current managed host is in.\ngroups : Lists all groups and hosts in the inventory.\ninventory_hostname : Contains the host name for the current managed host as configured in the inventory."
  },
  {
    "objectID": "Sysadmin3.html#implementing-task-control",
    "href": "Sysadmin3.html#implementing-task-control",
    "title": "5  Server Administrator III - RH294",
    "section": "5.4 Implementing Task Control",
    "text": "5.4 Implementing Task Control\n\n5.4.1 Loops\n\nSimple loop:\n\n- name: Postfix and Dovecot are running\n  service:\n    name: \"{{ item }}\"\n    state: started\n  loop:\n    - postfix\n    - dovecot\n\nThe list used by loop can be provided by a variable :\n\nvars:\n  mail_services:\n    - postfix\n    - dovecot\n\ntasks:\n  - name: Postfix and Dovecot are running\n    service:\n      name: \"{{ item }}\"\n      state: started\n    loop: \"{{ mail_services }}\"\n\nLoop over hash or Dict\n\n- name: Users exist and are in the correct groups\n  user:\n    name: \"{{ item.name }}\"\n    state: present\n    groups: \"{{ item.groups }}\"\n  loop:\n    - name: jane\n      groups: wheel\n    - name: joe\n      groups: root\n\nLoop Keywords :\n\nwith_items : The loop variable item holds the list item used during each iteration.\n\nvars:\n  data:\n    - user0\n    - user1\n    - user2\ntasks:\n  - name: \"with_items\"\n    debug:\n        msg: \"{{ item }}\"\n    with_items: \"{{ data }}\"\n\nwith_file : The loop variable item holds the content of a corresponding file from the file list during each iteration.\nwith_sequence : The loop variable item holds the value of one of the generated items in the generated sequence during each iteration\n\n\nUsing Register variable\n\nThe register keyword can also capture the output of a task that loops\n\n---\n- name: Loop Register Test\n  gather_facts: no\n  hosts: localhost\n  tasks:\n  \n    - name: Looping Echo Task\n      shell: \"echo This is my item: {{ item }}\"\n      loop:\n        - one\n        - two\n      register: echo_results\n\n    - name: Show echo_results variable\n      debug:\n        var: echo_results\n\n\n5.4.2 Task Conditionally\nAnsible can use conditionals to execute tasks or plays when certain conditions are met.\n\nBoolean condiction using when\n\n---\n- name: Simple Boolean Task Demo\n  hosts: all\n  vars:\n    run_my_task: true\n\n  tasks:\n    - name: httpd package is installed\n      yum:\n        name: httpd\n      when: run_my_task\n\nCondition to test if variable has a value\n\n---\n- name: Test Variable is Defined Demo\n  hosts: all\n  vars:\n    my_service: httpd\n\n  tasks:\n    - name: \"{{ my_service }} package is installed\"\n      yum:\n        name: \"{{ my_service }}\"\n      when: my_service is defined\n\nUsing data from Gathering Facts\n\n---\n- name: Demonstrate the \"in\" keyword\n  hosts: all\n  gather_facts: yes\n  vars:\n    supported_distros:\n      - RedHat\n      - Fedora\n\n  tasks:\n    - name: Install httpd using yum, where supported\n      yum:\n        name: http\n        state: present\n      when: ansible_distribution in supported_distros\n\n\n\nTesting multiple conditions\n\nwhen: ansible_distribution == \"RedHat\" or ansible_distribution == \"Fedora\"\n\n\n# or\n\nwhen: ansible_distribution_version == \"7.5\" and ansible_kernel == \"3.10.0-327.el7.x86_64\"\n\n\n# or\n\nwhen:\n- ansible_distribution_version == \"7.5\"\n- ansible_kernel == \"3.10.0-327.el7.x86_64\"\n\n\n# or\n\nwhen: &gt;\n  ( ansible_distribution == \"RedHat\" and\n    ansible_distribution_major_version == \"7\" )\n  or\n  ( ansible_distribution == \"Fedora\" and\n    ansible_distribution_major_version == \"28\" )\n\n\n\nLoop and conditions when is checking each item\n\nSample 1:\n- name: install mariadb-server if enough space on root\n  yum:\n    name: mariadb-server\n    state: latest\n  loop: \"{{ ansible_mounts }}\"\n  when: item.mount == \"/\" and item.size_available &gt; 300000000\nSample 2:\n---\n- name: Restart HTTPD if Postfix is Running\n  hosts: all\n  tasks:\n    - name: Get Postfix server status\n      command: /usr/bin/systemctl is-active postfix\n      ignore_errors: yes\n      register: result\n\n    - name: Restart Apache HTTPD based on Postfix status\n      service:\n        name: httpd\n        state: restarted\n      when: result.rc == 0\n\nregister: result : save info on the result variable\nwhen: result.rc == 0 : check the output of postfix task and restart httpd if systemctl command is 0\n\n\n\n5.4.3 Handlers\nHandlers are tasks that respond to a notification triggered by other tasks. Tasks only notify their handlers when the task changes something on a managed host.\nHandlers can be considered as inactive tasks that only get triggered when explicitly invoked using a notify statement.\n\nThe Apache server is only restarted by the restart apache handler when a configuration file is updated and notifies it.\n\ntasks:\n  - name: copy demo.example.conf configuration template\n    template:\n      src: /var/lib/templates/demo.example.conf.template\n      dest: /etc/httpd/conf.d/demo.example.conf\n    notify:\n      - restart apache\n\n\nhandlers:\n  - name: restart apache\n    service:\n      name: httpd\n      state: restarted\nManaging Task Errors in Play\n\nIgnoring task failure\n\n- name: Latest version of notapkg is installed\n  yum:\n    name: notapkg\n    state: latest\n  ignore_errors: yes\n\nForcing execution of handler after task failure using force_handlers\n\n---\n- hosts: all\n  force_handlers: yes\n  tasks:\n    - name: a task which always notifies its handler\n      command: /bin/true\n      notify: restart the database\n\n    - name: a task which fails because the package doesn't exist\n      yum:\n        name: notapkg\n        state: latest\n\n  handlers:\n    - name: restart the database\n      service:\n        name: mariadb           \n        state: restarted\n\nSpecify task failure\n\ntasks:\n  - name: Run user creation script\n    shell: /usr/local/bin/create_users.sh\n    register: command_result\n    failed_when: \"'Password missing' in command_result.stdout\"\n\nUsing fail module to force a task failure\n\ntasks:\n  - name: Run user creation script\n    shell: /usr/local/bin/create_users.sh\n    register: command_result\n    ignore_errors: yes\n\n  - name: Report script failure\n    fail:\n      msg: \"The password is missing in the output\"\n    when: \"'Password missing' in command_result.stdout\"\n\nSpecifying when a task reports “changed” results\n\ntasks:\n  - shell:\n      cmd: /usr/local/bin/upgrade-database\n    register: command_result        \n    changed_when: \"'Success' in command_result.stdout\"\n    notify:\n      - restart_database\n\nhandlers:\n  - name: restart_database\n    service:        \n      name: mariadb\n      state: restarted\n\n\n5.4.4 Blocks\n\nblocks are clauses that logically group tasks, and can be used to control how tasks are executed\n\n- name: block example\n  hosts: all\n  tasks:\n    - name: installing and configuring Yum versionlock plugin\n    \n      block:\n    \n      - name: package needed by yum\n        yum:\n          name: yum-plugin-versionlock\n          state: present\n    \n      - name: lock version of tzdata\n        lineinfile:\n          dest: /etc/yum/pluginconf.d/versionlock.list\n          line: tzdata-2016j-1\n          state: present\n     \n      when: ansible_distribution == \"RedHat\"\n\nUsing block , rescue and always\n\nblock: Defines the main tasks to run\nrescue: Defines the tasks to run if the tasks defined in the block clause fail.\nalways: Defines the tasks that will always run independently of the success or failure of tasks defined in the block and rescue clauses.\n\n\n\ntasks:\n    - name: Upgrade DB\n\n    block:\n      - name: upgrade the database\n        shell:\n          cmd: /usr/local/lib/upgrade-database\n\n    rescue:\n      - name: revert the database upgrade\n        shell:\n          cmd: /usr/local/lib/revert-database\n\n    always:\n      - name: always restart the database\n        service:\n          name: mariadb       \n        state: restarted"
  },
  {
    "objectID": "Sysadmin3.html#deploying-files-to-managed-hosts",
    "href": "Sysadmin3.html#deploying-files-to-managed-hosts",
    "title": "5  Server Administrator III - RH294",
    "section": "5.5 Deploying Files to Managed Hosts",
    "text": "5.5 Deploying Files to Managed Hosts\n\n5.5.1 Modifying and Copying Files to hosts\nFiles Modules\n\nblockinfile : Insert, update, or remove a block of multiline text surrounded by customizable marker lines\ncopy : Copy a file from the local or remote machine to a location on a managed host\nfetch : fetching files from remote machines to the control node and storing them in a file tree, organized by host name.\nfile : Create/Remove/Set attributes for (perm, ownership, SELinux) of files, links, dirs\nlineinfile : when you want to change a single line in a file.\nstat : Retrieve status information for a file, similar to the Linux stat command\nsynchronize : A wrapper around the rsync\n\n\nExamples\n\nEnsure file exists\n\n- name: Touch a file and set permissions\n  file:\n    path: /path/to/file\n    owner: user1\n    group: group1\n    mode: 0640\n    state: touch\n\nModify attributes\n\n- name: SELinux type is set to samba_share_t\n  file:\n    path: /path/to/samba_file\n    setype: samba_share_t\n\nMake SELinux file context persistent\n\n- name: SELinux type is persistently set to samba_share_t\n  sefcontext:\n    target: /path/to/samba_file\n    setype: samba_share_t\n    state: present\n\nCopy and Edit files on managed hosts\n\n- name: Copy a file to managed hosts\n  copy:\n    src: file\n    dest: /path/to/file\n\nTo retrieve files from managed hosts use the fetch module\n\n- name: Retrieve SSH key from reference host\n  fetch:\n    src: \"/home/{{ user }}/.ssh/id_rsa.pub\n    dest: \"files/keys/{{ user }}.pub\"\n\nTo ensure a specific single line of text exists in an existing file, using lineinfile\n\n- name: Add a line of text to a file\n  lineinfile:\n    path: /path/to/file\n    line: 'Add this line to the file'\n    state: present\n\nTo add a block of text to an existing file, using blockinfile\n\n- name: Add additional lines to a file\n  blockinfile:\n    path: /path/to/file\n    block: |\n      First line in the additional block of text\n      Second line in the additional block of text\n    state: present\n\nRemove files from managed hosts\n\n- name: Make sure a file does not exist on managed hosts\n  file:\n    dest: /path/to/file\n    state: absent\n\nRetrieving the status of a file on managed hosts\n\n- name: Verify the checksum of a file\n    stat:\n      path: /path/to/file\n      checksum_algorithm: md5\n    register: result\n\n- debug\n    msg: \"The checksum of the file is {{ result.stat.checksum }}\"\n\nUsing stat\n\n- name: Examine all stat output of /etc/passwd\n  hosts: localhost\n\n  tasks:\n    - name: stat /etc/passwd\n      stat:\n        path: /etc/passwd\n      register: results\n\n    - name: Display stat results\n      debug:\n        var: results\n\nSync files between control node and managed node\n\n- name: synchronize local file to remote files\n  synchronize:\n      src: file\n      dest: /path/to/file\n\n\n5.5.2 Deploying Custom files with Jinja2 templates\nJinja2 templates are a powerful tool to customize configuration files to be deployed on the managed hosts. When the Jinja2 template for a configuration file has been created, it can be deployed to the managed hosts using the template module.\n\nTo use template module\n\ntasks:\n  - name: template render\n    template:\n      src: /tmp/j2-template.j2\n      dest: /tmp/dest-config-file.txt\n\nLoops in Jinja2\n\n{% for user in users %}\n      {{ user }}\n{% endfor %}\n\nGenerate /etc/hosts files from host facts\n\n- name: /etc/hosts is up to date\n  hosts: all\n  gather_facts: yes\n  tasks:\n      - name: Deploy /etc/hosts\n        template:\n          src: templates/hosts.j2\n          dest: /etc/hosts\nThe below code templates/hosts.j2 template construct the file from all hosts in group all\n{% for host in groups['all'] %}\n{{ hostvars[host]['ansible_facts']['default_ipv4']['address'] }} {{ hostvars[host]\n['ansible_facts']['fqdn'] }} {{ hostvars[host]['ansible_facts']['hostname'] }}\n{% endfor %}\n\nUsing conditionals\n\n{% if finished %}\n{{ result }}\n{% endif %}"
  },
  {
    "objectID": "Sysadmin3.html#managing-complex-plays-and-playbooks",
    "href": "Sysadmin3.html#managing-complex-plays-and-playbooks",
    "title": "5  Server Administrator III - RH294",
    "section": "5.6 Managing Complex Plays and Playbooks",
    "text": "5.6 Managing Complex Plays and Playbooks\n\n5.6.1 Selecting Hosts with Host Patterns\n\nHost patterns are used to specify the hosts to target by a play or ad hoc command. We can use groups of hosts by [&lt;group_name&gt;] or group of groups [&lt;name&gt;:children] and specify the groups.\nWe can use the variable hosts inside the playbook :\n\nwe can affect all hosts add all or '*',\nUse the ungrouped\npart of names '*.exammple.com',\nfiltering ip 192.168.2.*\nBy part of groups 'datacenter*' or 'data*'\nLists servera, serverb,192.168.2.2\nMixed 192.168.2*, lab, data*\nUsing AND logical operator lab, $datacenter1\nUsing NOT logical operator lab , !datacenter1\n\n\n\n\n5.6.2 Including and Importing Files\nThere are two options to bring contenr into a playbook, using include or import\n\nInclude : it is a dynamic operation. Ansible processes included content during the run of the playbook, as content is reached.\nImport : it is a static operation. Ansible preprocesses imported content when the playbook is initially parsed, before the run starts.\n\nCannot use loops\n\n\n- name: Prepare the web server\n  import_playbook: web.yml\n\n\n- name: Prepare the database server\n  import_playbook: db.yml\nWe can create a task file and import that task file\n\ntask file\n\n[admin@node ~]$ cat webserver_tasks.yml\n- name: Installs the httpd package\n  yum:\n      name: httpd\n      state: latest\n\n- name: Starts the httpd service\n  service:\n      name: httpd\n      state: started\n\nImporting a task file\n\n---\n- name: Install web server\n  hosts: webservers\n  tasks:\n  - import_tasks: webserver_tasks.yml"
  },
  {
    "objectID": "Sysadmin3.html#simplifying-playbooks-with-roles",
    "href": "Sysadmin3.html#simplifying-playbooks-with-roles",
    "title": "5  Server Administrator III - RH294",
    "section": "5.7 Simplifying Playbooks with Roles",
    "text": "5.7 Simplifying Playbooks with Roles\nAnsible roles have the following benefits:\n\nRoles group content, allowing easy sharing of code with others\nRoles can be written that define the essential elements of a system type: web server, database server, Git repository, or other purpose\nRoles make larger projects more manageable\nRoles can be developed in parallel by different administrators\n\n\n5.7.1 Describing Role Structure\n\nSubdirectories\n\ndefaults : DEfault value of role variables, low precedence\nfiles : static files that are referenced by role tasks\nhandlers : role’s handler definitions\nmeta : info about role, author, license, platforms, etc\ntasks : role’s task definitions\ntemplates : Jinja2 templates that are referenced by role tasks\ntests : can contain an inventory and test.yml playbook for test\nvars : define roleś variables, high precedence\n\n\nUsing Roles\n---\n- hosts: remote.example.com\n roles:\n   - role1\n   - role2\nControlling Order of Execution\nThe following play shows an example with pre_tasks, roles, tasks, post_tasks and handlers. It is unusual that a play would contain all of these sections\n- name: Play to illustrate order of execution\n  hosts: remote.example.com\n  \n  pre_tasks:\n    - debug:\n        msg: 'pre-task'\n      notify: my handler\n\n\n  roles:\n    - role1\n  \n  tasks:\n    - debug:\n        msg: 'first task'\n      notify: my handler\n\n  post_tasks:\n    - debug:\n        msg: 'post-task'\n      notify: my handler\n\n  handlers:\n    - name: my handler\n      debug:\n        msg: Running my handler\nUsing include_role\n- name: Execute a role as a task\n  hosts: remote.example.com\n  tasks:\n      - name: A normal task\n        debug:\n            msg: 'first task'\n      - name: A task to include role2 here\n        include_role: role2\n\n\n5.7.2 Reusing Content with System Roles\nRHEL System Roles\n\nrhel-system-roles.kdump : configure kdump\nrhel-system-roles.network : configure network interfaces\nrhel-system-roles.selinux : Configure and manage SELinux\nrhel-system-roles.timesync : Confgure time sync using NTP or PTP\nrhel-system-roles.postfix : Configure each host as a mail transfer agent using postfix\nrhel-system-roles.firewall : configure firewall\nrhel-system-roles.tuned : configure tuned service\nInstall RHEL Roles, after install the role are located at /usr/share/ansible/roles/\n\n[root@host ~]# yum install rhel-system-roles\n\nTime Sync Role Example\n\n- name: Time Synchronization Play\n  hosts: servers\n  vars:\n    timesync_ntp_servers:\n      - hostname: 0.rhel.pool.ntp.org\n        iburst: yes\n      - hostname: 1.rhel.pool.ntp.org\n        iburst: yes\n      - hostname: 2.rhel.pool.ntp.org\n        iburst: yes\n    timezone: UTC\n\n  roles:\n      - rhel-system-roles.timesync\n\n  tasks:\n      - name: Set timezone\n        timezone:\n            name: \"{{ timezone }}\"\n\n\n5.7.3 Creating Roles\nProcess\n\nCreate the role directory structure.\nDefine the role content.\nUse the role in a playbook.\n\n\nDirectory Structure\nBy default, Ansible looks for roles in a subdirectory called roles in the directory containing your Ansible Playbook, if cannot find the role, it looks at roles_path\n~/.ansible/roles:/usr/share/ansible/roles:/etc/ansible/roles\nThe ansible-galaxy command line tool s used to manage Ansible roles, including the creation of new roles.\ncd roles\nansible-galaxy init my_new_role\n\n\nRecommended Practices for Role Content Development\n\nMaintain each role in its own version control repository. Ansible works well with git-based repositories\nSensitive information, such as passwords or SSH keys, should not be stored in the role repository\nUse ansible-galaxy init to start your role\nCreate and maintain README.md and meta/main.yml files to document what your role is for, who wrote it, and how to use it\nKeep your role focused on a specific purpose or function\nReuse and refactor roles often. Resist creating new roles for edge configurations\n\n\nRole Dependencies\nDependencies are defined in the meta/main.yml file in the role directory hierarchy.\n---\ndependencies:\n  - role: apache\n      port: 8080\n  - role: postgres\n      dbname: serverlist\n      admin_user: felix\n\nUsing Role in playbook\nTo access a role, reference it in the roles: section of a play.\n[user@host ~]$ cat use-motd-role.yml\n---\n- name: use motd role playbook\n  hosts: remote.example.com\n  remote_user: devops\n  become: true\n  roles:\n    - motd\nWhen the playbook is executed, tasks performed because of a role can be identified by the role name prefix\n[user@host ~]$ ansible-playbook -i inventory use-motd-role.yml\n\nPLAY [use motd role playbook] **************************************************\n\nTASK [setup] *******************************************************************\nok: [remote.example.com]\n\nTASK [motd: deliver motd file] ************************************************\nchanged: [remote.example.com]\n\nPLAY RECAP *********************************************************************\nremote.example.com        : ok=2        changed=1       unreachable=0     failed=0\n\n\n5.7.4 Deploying Roles with Ansible Galaxy\nAnsible Galaxy https://galaxy.ansible.com is a public library of Ansible content written by a variety of Ansible administrators and users.\nUsing Ansible Galaxy Command-Line tool\n\nSearch\n\n[user@host ~]$ ansible-galaxy search 'redis' --platforms EL\n\nGet info\n\n[user@host ~]$ ansible-galaxy info geerlingguy.redis\n\nInstall from Ansible Galaxy\n\n[user@host project]$ ansible-galaxy install geerlingguy.redis -p roles/\n\nInstall using requirements.yml\n\n[user@host project]$ ansible-galaxy install -r roles/requirements.yml  -p roles\n\nList the roles locally\n\n[user@host project]$ ansible-galaxy list\n\nRemove role\n\n[user@host ~]$ ansible-galaxy remove nginx-acme-ssh\n\n\n5.7.5 Roles and Modules from Content Collections\nAnsible content collections are a distribution format for Ansible content. A collection provides a set of related modules, roles, and plug-ins that you can download to your control node and then use in your playbooks.\n\ninstall Contect collections\n\n[user@controlnode ~]$ ansible-galaxy collection install community.crypto\n\nInstall from collection path\n\n[root@controlnode ~]# ansible-galaxy collection install  -p /usr/share/ansible/collections community.postgresql\n\nInstall from tar file\n\n[user@controlnode ~]$ ansible-galaxy collection install /tmp/community-dns-1.2.0.tar.gz\n\n\n[user@controlnode ~]$ ansible-galaxy collection install http://www.example.com/redhat-insights-1.0.5.tar.gz\n\nInstall from requirements.yml file\n\n---\ncollections:\n  - name: community.crypto\n\n  - name: ansible.posix\n    version: 1.2.0\n\n  - name: /tmp/community-dns-1.2.0.tar.gz\n\n    - name: http://www.example.com/redhat-insights-1.0.5.tar.gz\n    \n    \n    \n[root@controlnode ~]# ansible-galaxy collection install -r requirements.yml"
  },
  {
    "objectID": "Sysadmin3.html#troubleshooting-ansible",
    "href": "Sysadmin3.html#troubleshooting-ansible",
    "title": "5  Server Administrator III - RH294",
    "section": "5.8 Troubleshooting Ansible",
    "text": "5.8 Troubleshooting Ansible\n\n5.8.1 Troubleshooting Playbooks\nBy default, Ansible is not configured to log its output to any log file. We can configure the log_path variale in ansible.cfg or using $ANSIBLE_LOG_PATH\nRelate debug we can increase the verbosity of output using -vvvv or inside the playbook using verbosity parameter\nname: Display the \"output\" variable\n  debug:\n    var: output\n    verbosity: 2\n\nManaging Errors\n\nWe can use --syntax-check to verify the YAML syntax\nWe can also use the --step to step through a playbook one task at a time\n\n[student@demo ~]$ ansible-playbook play.yml --step\n\nWe also can star the playbook in a specific task using --start-at-task\n\n[student@demo ~]$ ansible-playbook play.yml --start-at-task=\"start httpd service\"\n\nRecommended Practices for Playbook Management\n\nUse a concise description of the play’s or task’s purpose to name plays and tasks\nInclude comments to add additional inline documentation about tasks.\nOrganize task attributes vertically to make them easier to read.\nConsistent horizontal indentation is critical. Use spaces, not tabs, to avoid indentation errors.\nTry to keep the playbook as simple as possible. ### Troubleshooting Managed Hosts\n\nWe can use the ansible-playbook --check command to run smoke tests on a playbook. This option executes the playbook without making changes to the managed hosts’ configuration.\n[student@demo ~]$ ansible-playbook --check playbook.yml\nAnother alternative is use --check and --diff, this option reports the changes made to the template files on managed hosts, those changes are displyed in the command but not actually made\n[student@demo ~]$ ansible-playbook --check --diff playbook.yml"
  },
  {
    "objectID": "Sysadmin3.html#automating-linux-administration-tasks",
    "href": "Sysadmin3.html#automating-linux-administration-tasks",
    "title": "5  Server Administrator III - RH294",
    "section": "5.9 Automating Linux Administration Tasks",
    "text": "5.9 Automating Linux Administration Tasks\n\n5.9.1 Managing Software and Subscriptions\nWe can use the yum module to manage package on manage hosts\n\nInstall :\n\n---\n- name: Install the required packages on the web server\n  hosts: servera.lab.example.com\n  tasks:\n    - name: Install the httpd packages\n      yum:\n        name: httpd\n        state: present\n\nUpdate all packages, on this case we need to use whild card *\n\n- name: Update all packages\n  yum:\n     name: '*'\n      state: latest\n\nInstall group, _must prefix group names with (_?)\n\n- name: Install Development Tools\n  yum:\n    name: '@Development Tools'\n    state: presen\n\nInstall module, _must use the module name with prefix (_?)\n\n- name: Inst perl AppStream module\n  yum:\n    name: '@perl:5.26/minimal'\n    state: present\n\nOptimizing Multiple Package Installation, using list is more efficient then loop\n\n---\n- name: Install the required packages on the web server\n  hosts: servera.lab.example.com\n  tasks:\n    - name: Install the packages\n      yum:\n        name:\n          - httpd\n          - mod_ssl\n          - httpd-tools\n        state: present\n\nGathering Facts about Installed Packages\n\nThe package_facts Ansible module collects the installed package details on managed hosts\n---\n- name: Display installed packages\n  hosts: servera.lab.example.com\n  tasks:\n    - name: Gather info on installed packages\n      package_facts:\n        manager: auto\n        \n    - name: List installed packages\n      debug:\n        var: ansible_facts.packages\n\n    - name: Display NetworkManager version\n      debug:\n        msg: \"Version {{ansible_facts.packages['NetworkManager'][0].version}}\"\n      when: \"'NetworkManager' in ansible_facts.packages\"    \n\nTo register a Subscription\n\n- name: Register and subscribe the system\n  redhat_subscription:\n    username: yourusername\n    password: yourpassword\n    pool_ids: poolID\n    state: present\n\nTo enabling Red Hat Software Repositories\n\n- name: Enable Red Hat repositories\n  rhsm_repository:\n    name:\n      - rhel-8-for-x86_64-baseos-rpms\n      - rhel-8-for-x86_64-baseos-debug-rpms\n    state: present\n\n\n5.9.2 Managing Users and Authenticatin\nUser Module\n\nAdd New user\n\n- name: Add new user to the development machine and assign the appropriate groups.\n  user:\n    name: devops_user\n    shell: /bin/bash\n    groups: sys_admins, developers\n    append: yes\n\nAdd new user generating an ssh key\n\nname: Create a SSH key for user1\n  user:\n    name: user1\n    generate_ssh_key: yes\n    ssh_key_bits: 2048\n    ssh_key_file: .ssh/id_my_rsa\n\nGroup Module\n\nVerify a group module\n\n- name: Verify that auditors group exists\n  group:\n    name: auditors\n    state: present\n\nKnown Hosts Module\n\nCopy host key\n\n- name: copy host keys to remote servers\n  known_hosts:\n    path: /etc/ssh/ssh_known_hosts\n    name: host1\n    key: \"{{ lookup('file', 'pubkeys/host1') }}\"\n\nAuthorized Key Module\nSet authorized key\n- name: Set authorized key\n  authorized_key:\n    user: user1\n    state: present\n    key: \"{{ lookup('file', '/home/user1/.ssh/id_rsa.pub') }}\n\n\n5.9.3 Managing the Boot Process and Scheduled Processes\n\nWe can schedule jobs with at or cron\n\n- name: remove tempuser.\n  at:\n    command: userdel -r tempuser\n    count: 20\n    units: minutes\n    unique: yes\n    \n- cron:\n  name: \"Flush Bolt\"\n  user: \"root\"\n  minute: 45\n  hour: 11\n  job: \"php ./app/nut cache:clear\"\n\nManage service with systemd and service\n\n- name: start nginx\n  service:\n    name: nginx\n    state: started\"\n- name: reload web server\n  systemd:\n    name: apache2\n    state: reload\n    daemon-reload: yes\n\nReboot\n\n- name: \"Reboot after patching\"\n  reboot:\n    reboot_timeout: 180\n\n- name: force a quick reboot\n  reboot:\n\nUse Shell and Command\n\n- name: Run a templated variable (always use quote filter to avoid injection)\n    shell: cat {{ myfile|quote }}   \n- name: This command only\n  command: /usr/bin/scrape_logs.py arg1 arg2\n  args:\n    chdir: scripts/\n    creates: /path/to/script\n\n\n5.9.4 Managing Storage\n\nConfigure using parted\n\nname: New 10GB partition\n  parted:\n    device: /dev/vdb\n    number: 1\n    state: present\n    part_end: 10GB\nUsing lvg and lvol modules\n\nCreate volume group\n\n- name: Creates a volume group\n  lvg:\n    vg: vg1\n    pvs: /dev/vda1\n    pesize: 32\n\nResize volume group\n\n- name: Resize a volume group\n  lvg:\n    vg: vg1\n    pvs: /dev/vdb1,/dev/vdc1\n\nCreate logical volume\n\nname: Create a logical volume of 2GB\n  lvol:\n    vg: vg1\n    lv: lv1\n    size: 2g\n\nCreate XFS filesytem\n\n- name: Create an XFS filesystem\n  filesystem:\n    fstype: xfs\n    dev: /dev/vdb1\n\nMount\n\nMount device with ID\n\n- name: Mount device with ID\n  mount:\n    path: /data\n    src: UUID=a8063676-44dd-409a-b584-68be2c9f5570\n    fstype: xfs\n    state: present\n\nMount NFS\n\n- name: Mount NFS share\n  mount:\n    path: /nfsshare\n    src: 172.25.250.100:/share\n    fstype: nfs\n    opts: defaults\n    dump: '0'\n    passno: '0'\n    state: mounted\n\nConfigure Swap with Modules\n- name: Create new swap VG\n  lvg:\n    vg: vgswap\n    pvs: /dev/vda1\n    state: present\n\n- name: Create new swap LV\n  lvol:\n    vg: vgswap\n    lv: lvswap\n    size: 10g\n\n- name: Format swap LV\n  command: mkswap /dev/vgswap/lvswap\n  when: ansible_swaptotal_mb &lt; 128\n\n- name: Activate swap LV\n  command: swapon /dev/vgswap/lvswap\n  when: ansible_swaptotal_mb &lt; 128\n\nAnsible Facts for Storage Config\nWe can use ansible setup to retrieve all the facts for manage hosts\n[user@controlnode ~]$ ansible webservers -m setup\n\nor\n\n[user@controlnode ~]$ ansible webservers -m setup -a 'filter=ansible_devices'\n\nor\n\n\n[user@controlnode ~]$ ansible webservers -m setup -a 'filter=ansible_device_links'\n\n\nor\n\n[user@controlnode ~]$ ansible webservers -m setup -a 'filter=ansible_mounts'\n\n\n5.9.5 Managing Network Configuration\nConfiguring Network with Network System Role\n\nlist the currently installed system roles with the ansible-galaxy list command.\n\n[user@controlnode ~]$ ansible-galaxy list\nRoles are located in the /usr/share/ansible/roles directory. The network role is configured with two variables, network_provider and network_connections.\n\nThe network_provider variable configures the back end provider, either nm (NetworkManager) or initscripts. On Red Hat Enterprise Linux 8, the network role uses the nm (NetworkManager) as a default networking provider\n\n---\nnetwork_provider: nm\nnetwork_connections:\n  - name: ens4\n    type: ethernet\n    ip:\n      address:\n        - 172.25.250.30/24\n\nThe network_connections variable configures the different connections, specified as a list of dictionaries, using the interface name as the connection name.\n\nnetwork_connections:\n- name: eth0\n  persistent_state: present\n  type: ethernet\n  autoconnect: yes\n  mac: 00:00:5e:00:53:5d\n  ip:\n    address:\n    - 172.25.250.40/24\n  zone: external\n\nTo use the network system role, you need to specify the role name under the roles clause in your playbook as follows:\n\n- name: NIC Configuration\n  hosts: webservers\n  vars:\n    network_connections:\n      - name: ens4\n        type: ethernet\n        ip:\n          address:\n            - 172.25.250.30/24\n  roles:\n    - rhel-system-roles.network\n\nConfigure Network with modules\nThe nmcli module supports the management of both network connections and devices\n- name: NIC configuration\n  nmcli:\n    conn_name: ens4-conn\n    ifname: ens4\n    type: ethernet\n    ip4: 172.25.250.30/24\n    gw4: 172.25.250.1\n    state: present\n\nHostname module\n\nChange the hostname with hostname module\n\nname: Change hostname\n  hostname:\n    name: managedhost1\n\nFirewalld module\nThe firewalld module supports the management of FirewallD on managed hosts\n- name: Enabling http rule\n  firewalld:\n    service: http\n    permanent: yes\n    state: enabled\n\nConfigures the eth0 in the external FirewallD zone.\n\n- name: Moving eth0 to external\n  firewalld:\n    zone: external\n    interface: eth0\n    permanent: yes\n    state: enabled\n\nAnsible Facts for Network Configuration\n[user@controlnode ~]$ ansible webservers -m setup\n\nor\n\n[user@controlnode ~]$ ansible webservers -m setup  -a 'gather_subset=network filter=ansible_interfaces'\n\nor\n\n[user@controlnode ~]$ ansible webservers -m setup -a 'gather_subset=network filter=ansible_ens4'"
  },
  {
    "objectID": "OpenShift1.html#intro-to-container",
    "href": "OpenShift1.html#intro-to-container",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.1 Intro to Container",
    "text": "6.1 Intro to Container\nContainers ares a set of one or more processes that are isolated from the rest of the system.\nContainers provide many of the same benefits as virtual machines, such as security, storage, and network isolation. Containers require far fewer hardware resources and are quick to start and terminate. They also isolate the libraries and the runtime resources (such as CPU and storage) for an application to minimize the impact of any OS update to the host OS.\n\n\n\n\n\nAdvantages of using container:\n\nLow hardware footprint : Use OS internal features to create and isolate environment, minimizing the use of cpu and memory\nEnvironment isolation : changes made to the host OS do not affect the container\nQuick deployment : no need to install the entire OS\nMultiple environment deployment : all appls dependencies and environment settings are encapsulated in the container image\nReusability : container can be reused without need to set up a full OS\n\n\n6.1.1 Linux Conetainer Architecture\nAn image is a template for containers that include a runtime environment and all the libs are configuration files, container images need to be locally available for the container runtime, below image repositories available :\n\nRed Hat Container Catalog : https://registry.redhat.io\nDocker Hub : https://hub.docker.com\nRed Hat Quay : https://quay.io\nGoogle Container Registry : https://cloud.google.com/container-registry/\nAmazon Elastic Container Registry : https://aws.amazon.com/ecr/\n\nTo manage the container we can use podman an open source tool for managing containers and container image\nif need to install podman :\nyum install podman\n\n\n6.1.2 Overview of kubernetes and OpenShift\nThink : as the number of containers managed by an organization grows, the work of manually starting them rises exponentially along with the need to quickly respond to external demands\nEnterprise needs:\n\nEasy communication between a large number of services\nResources limits on applications regardless of the number fo containers running them\nTo respond to application usage spikes to increase or decrease running containers\nReacs to service deterioration with health checks\nGradual roll out fo a new release to a set of users\n\nKubernetes is an orchestration service that simplifies the deployment, management, and scaling of containerized applications, the smallest unit if kunernetes is a pod that consist of one or more containers.\nKubernetes features of top of a container infra:\n\nService discovery and loading balancing : communication by a single DNS entry to each set of container, permits the load balancing across the pool of container.\nHorizontal scaling : Appl can scale up and down manually or automatically\nSelf-Healing: user-defined health checks to monitor containers to restart in case of failure\nAutomated rollout and rollback : roll updates out to appl containers, if something goes wrong kubernetes can rollback to previous integration of the deployment\nSecrets and configuration management : can manage the config settings fo application without rebuilding container\nOperators : use API to update the cluster state reacting to change in the app state\n\nRed Hat OpenShift Container Plataform (RHOCP) is a set of modular components and services build on top of Kubernetes, adds the capabilities to provide PaaS platform.\nOpenShift features to kubernetes cluster :\n\nItegrated developer workflow : integrates a build in container registry, CI/CD pipeline and S2I, a tool to build artifacts from source repositories to container image\nRoutes : expose service to the outside world\nMetrics and logging : Metric service and aggregated logging\nUnified UI : UI to manage the different capabilities"
  },
  {
    "objectID": "OpenShift1.html#creating-containerized-services",
    "href": "OpenShift1.html#creating-containerized-services",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.2 Creating Containerized Services",
    "text": "6.2 Creating Containerized Services\nThe podman is designed to be a rootless container running as a non-root user, however we can run the container as root if necessary using sudo, but it is a risk and not recommenced.\nThe container image are named based on the syntax registry_name/user_name/image_name:tag\n\nregistry_name : FQDN or the registry\nuser_name : name of user or organization to which images belongs\ntag : identifies image version\n\nBasic Commands:\n\nTo search an image\n\npodman search &lt;image&gt;\n\nTo download/pull an image\n\npodman pull &lt;image&gt;\n\nTo retrieve the images\n\npodman images\n\nTo run a Hello World container\n\n[user@demo ~]$ podman run ubi8/ubi:8.3 echo 'Hello world!'\nHello world!\n\nTo start a container image as a background we can use -d option and to expose a port -p &lt;container port&gt;\n\n[user@demo ~]$ podman run -d -p 8080 registry.redhat.io/rhel8/httpd-24\n\n# retrieve the local port on which the container listens\n[user@demo ~]$ podman port -l\n\n\n\n# test\n[user@demo ~]$ curl http://0.0.0.0:44389\n\n\n\nTo start a bash terminal inside the container\n\n[user@demo ~]$ podman run -it ubi8/ubi:8.3 /bin/bash\n\nUsing variables with -e option\n\n[user@demo ~]$ podman run --name mysql-custom \\\n&gt; -e MYSQL_USER=redhat -e MYSQL_PASSWORD=r3dh4t \\\n&gt; -e MYSQL_ROOT_PASSWORD=r3dh4t \\\n&gt; -d registry.redhat.io/rhel8/mysql-80\n\n\n\n# test\n\nsudo podman exec -ti msql-custom /bin/bash\nmsql -uroot\nshow databases\n\n\n\nGive a container a name --name, it is important because you can manage your container by name\nOther common option is -t for pseudo terminal an -i keeps stdin open even if not attached"
  },
  {
    "objectID": "OpenShift1.html#managing-containers",
    "href": "OpenShift1.html#managing-containers",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.3 Managing Containers",
    "text": "6.3 Managing Containers\n\n6.3.1 Container Life Cybe management with podman\nPodman provides a set of subcomands to create and manage containers ?\n\n\n\n\n\nAlso subcommands to extract information from containers ?\n\n\n\n\n\n\n\n6.3.2 Creating containers\nUsing podman run command to create containers\n# sample 1\n[user@host ~]$ podman run registry.redhat.io/rhel8/httpd-24\n\n# sample 2\n[user@host ~]$ podman run --name my-httpd-container -d registry.redhat.io/rhel8/httpd-24\n\n\n# sammple 3\n[user@host ~]$ podman run -it registry.redhat.io/rhel8/httpd-24 /bin/bash\n\n\n\n6.3.3 Run commands in a container\nWe can use exec option to submit the command\nsample 2\n[user@host ~]$ podman exec 7ed6e671a600 cat /etc/hostname\n\n# sample 2  this l means latest, last container used\n[user@host ~]$ podman exec -l cat /etc/hostname\n\n\n6.3.4 Managing containers\n\nList containers running :\n\npodman ps\n\nList all containers\n\npodman ps -a\n\nStop, start or restart a container\n\n[user@host ~]$ podman stop|start|restart &lt;container_name&gt;\n\nKill or remove a container\n\n[user@host ~]$ podman rm|kill &lt;container_name&gt;\n\n\nRemove or stop all containers\n\npodman rm|stop -a\n\nFormat the output\n\n[student@workstation ~]$ podman ps --format=\"{{.ID}} {{.Names}} {{.Status}}\"\na49dba9ff17f  mysql Up About a minute ago\n\n\n6.3.5 Attaching persistent storage to containers\n\nCreate dir\n\nmkdir &lt;dir&gt;\n\nThe user running the process in the container must be capable of writing files to the dir, for example in MYSQL the UID 27\n\npodman unshare chown -R 27:27 &lt;dir&gt;\n\nApply the container_file_t context to allow container access\n\nsudo semanage fcontext -a -t container_file_t '/home/student/dbfiles(/.*)?' \n\nApply the SELinux container policy\n\nsudo restorecon -Rv /home/student/dbfiles\n\nMount volume\n\n[user@host ~]$ podman run -v /home/student/dbfiles:/var/lib/mysql rhmap47/mysql\n\n\n6.3.6 Accessing containers\nTo manage the port we use the option -p [&lt;IP address&gt;:][&lt;host port&gt;:]&lt;container port&gt;\npodman run -d --name apache1 -p 8080:80 registry.redhat.io/rhel8/httpd-24\n\nTo see the port assigned\n\npodman port &lt;container name&gt;"
  },
  {
    "objectID": "OpenShift1.html#managing-container-images",
    "href": "OpenShift1.html#managing-container-images",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.4 Managing Container Images",
    "text": "6.4 Managing Container Images\n\n6.4.1 Accessing Registries\nImage registries are services offering container images to download. They allow image creators and maintainers to store and distribute container images to public or private audiences.\nTo configure registreis for podman command we need to update /etc/containers/registries.conf\n\nTo search :\n\n[user@host ~]$ podman search [OPTIONS] &lt;term&gt;\n\nTo authenticate\n\npodman login &lt;registry&gt;\n\nPull images\n\n[user@host ~]$ podman pull [OPTIONS] [REGISTRY[:PORT]/]NAME[:TAG]\n\n[user@host ~]$ podman pull quay.io/bitnami/nginx\n\nList local copies\n\npodman images\n\nImages Tags\n\nAn image tag is a mechanism to support multiple releases of the same image\n[user@host ~]$ podman pull registry.redhat.io/rhel8/mysql-80:1\n\n[user@host ~]$ podman run registry.redhat.io/rhel8/mysql-80:1\n\n\n6.4.2 Manipulating Container Images\n\nSave and load an image\n\nImages can be saved as .tar file :\n# Save\npodman save [-o FILE_NAME] IMAGE_NAME[:TAG]\npodman save -o mysql.tar registry.redhat.io/rhel8/mysql-80\n\n\n#Load\npodman load [-i FILE_NAME]\npodman load -i mysql.tar\n\n\nDelete an image from local storage\n\npodman rmi [OPTIONS] IMAGE [IMAGE...]\n\n# To delete all\npodman rmi -a\n\nTo modify an image\n\n[user@host ~]$ podman commit [OPTIONS] CONTAINER [REPOSITORY[:PORT]/]IMAGE_NAME[:TAG]\nTo see the difference that we have made on container we can use podman diff &lt;image&gt;\n\nTo commit changes\n\n[user@host ~]$ podman commit mysql-basic  mysql-custom\n\nTagging Images\n\npodman tag [OPTIONS] IMAGE[:TAG]  [REGISTRYHOST/][USERNAME/]NAME[:TAG]\n\n# sample 1\npodman tag mysql-custom devops/mysql\n\n# sample 2\npodman tag mysql-custom devops/mysql:snapshot\n\nPush images to registry\n\n[user@host ~]$ podman push [OPTIONS] IMAGE [DESTINATION]\n\n# sample\n[user@host ~]$ podman push quay.io/bitnami/nginx\n\nTo remove tags from image\n\npodman rmi devops/mysql:snapahot"
  },
  {
    "objectID": "OpenShift1.html#creating-custom-container-images",
    "href": "OpenShift1.html#creating-custom-container-images",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.5 Creating Custom Container Images",
    "text": "6.5 Creating Custom Container Images\n\n6.5.1 Designing Custom Container Images\nOne method to create a container image is modify the existing one to meet the requirements of the application. Containerfiles are another option that make this task easy to create, share and control the image.\n\nRed Hat Software Collections Library (RHSCL) : solution for developers who require the latest development tools that usually do not fit the standard RHEL release schedule.\nRed Hat Enterprise Linux (RHEL) : stable environment for enterprise applications.\n\nWe can finding Containerfiles from Red HAt Collections Library, RHSCL is the source of most container images provided by the Red Hat image registry for use by RHEL Atomic Host and OpenShift Container Platform customers.\nAlso Red Hat Container Catalog RHCC is a repository of reliable, tested, certified, and curated collection of container images built on versions of Red Hat Enterprise Linux (RHEL) and related systems\nQuay.io is an advanced container repository from CoreOS, we can search for container images using httpds://quay.io/search\nDocker Hub is a repository that anyone can crete and share an image, need to be carreful with images from Docker Hub\nSource-to-Image (S2I) the OpenShift source-to-image tool is an alternative to using Containerfiles to create new containers that can be use from OpenShift or as standalone s2i utility, The S2I use the follow process to build a custom container image:\n\nStart a container from a base container image called the builder image.\nFetch the application source code, usually from a Git server, and send it to the container.\nBuild the application binary files inside the container.\nSave the container, after some clean up ### Building Custom Container images with Containerfiles\n\nA Containerfile is a mechanism to automate the building of container images, to build we have three steps:\n\nCreate a working directory\nWrite the Containerfile\nBuild the image with Podman\n\n# This is a comment line\nFROM ubi8/ubi:8.3\nLABEL description=\"This is a custom httpd container image\"\nMAINTAINER John Doe &lt;jdoe@xyz.com&gt;\nRUN yum install -y httpd\nEXPOSE 80\nENV LogLevel \"info\"\nADD http://someserver.com/filename.pdf /var/www/html\nCOPY ./src/ /var/www/html/\nUSER apache\nENTRYPOINT [\"/usr/sbin/httpd\"]\nCMD [\"-D\", \"FOREGROUND\"]\n\nFROM : declares that the new container image extends ubi8/ubi:8.3 container base image\nLABEL: is responsible for adding generic metadata to an image\nMAINTAINER : Indicates the author\nRUN : executes commands in a new layer on top of the current image\nEXPOSE : indicates that the container listens on the specified network port at runtime\nENV : is responsible for defining environment variables that are available in the container\nADD : copies files or folders from a local or remote source and adds them to the container’s file system, ADD also unpack local .tar files\nCOPY : copies files from the working directory and adds them to the container’s file system\nUSER : specifies the username or the UID to use when running the container image for the RUN, CMD, and ENTRYPOINT instructions\nENTRYOINT : specifies the default command to execute when the image runs in a container.\nCMD : provides the default arguments for the ENTRYPOINT instruction\n\nBuilding Image with Podman\nPodman build command process the Containerfile and build a new image\npodman build -t NAME:TAG DIR\nImage layering\n\nEach instruction in a Containerfile create a new layer\n\nSample\nCreating a container file\nFROM rhel7:7.5\n\nMAINTAINER Bruno Machado &lt;bmachado@kyndryl.com&gt;\n\nLABEL description =  \"A custom Apache image\"\n\nADD training.repo /etc/yum.repos.d/training.repo\n\nRUN yum install -y htppd && \\\n    yum clean all\n    \nRUN echo \"Hello  from  Containerfile \" &gt; /usr/share/httpd/noindex/index.html\n\nEXPOSE 80\n\nENTRYPOINT [\"httpd\",\"-D\",\"FOREGROUND\"]\nBuild\n# Build\nsudo podman build -t d080/apache .\n\n# Check images:\nsudo pdman images\n\n# Run the container\nsudo podman run --name lab-apache -d -p 10080:80 do080/apache\n\n\n# Test with curl\ncurl 127.0.0.1:10080"
  },
  {
    "objectID": "OpenShift1.html#deploying-containerized-applications-on-openshift",
    "href": "OpenShift1.html#deploying-containerized-applications-on-openshift",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.6 Deploying Containerized Applications on OpenShift",
    "text": "6.6 Deploying Containerized Applications on OpenShift\n\n6.6.1 Describing Kubernetes and OpenShift Architecture\nKubernetes and OpenShift\n\nKubernetes is an orchestration service that simplifies the deployment, management, and scaling of containerized applications.\n\n\nKubernetes Terminology :\n\nNode : A server that hosts applications in a Kubernetes cluster.\nControl Plane : Provides basic cluster services such as APIs or controllers.\nCompute Node : This node executes workloads for the cluster. Application pods are scheduled onto compute nodes.\nResource : kind of component definition managed by Kubernetes. Resources contain the configuration of the managed component and the current state of the component\nController : A controller is a Kubernetes process that watches resources and makes changes attempting to move the current state towards the desired state.\nLabel : A key-value pair that can be assigned to any Kubernetes resource. Selectors use labels to filter eligible resources for scheduling and other operations.\nNamespace : A scope for Kubernetes resources and processes, so that resources with the same name can be used in different boundaries.\n\n\n\nRed Hat OpenShift Container Platform is a set of modular components and services built on top of Red Hat CoreOS and Kubernetes. RHOCP adds PaaS capabilities such as remote management, increased security, monitoring and auditing, application lifecycle management, and self-service interfaces for developers.\n\n\nOpenShift Terminology\n\nInfra Node : A node server containing infrastructure services like monitoring, logging, or external routing\nConsole : A web UI provided by the RHOCP cluster that allows developers and administrators to interact with cluster resources\nProject : OpenShift extension of Kubernetes’ namespaces. Allows the definition of user access control (UAC) to resources.\n\n\n\n\n\n\n\n\nCoreOS is a Linux distribution focused on providing an immutable operating system for container execution\nCRI-O is an implementation of the Kubernetes Container Runtime Interface CRI\nKubernetes manages a cluster of hosts, physical or virtual that run containers.\nEtcd : Key-value store to store config and state information about container and other resources\nCRD C_ustom Resource Definition_ are resource types stored in Etcd and managed by Kubernetes\nContainerized services fulfill many PaaS infrastructure functions, such as networking and authorization.\nRuntimes and xPaaS based container images ready for use for dev\nRHOCP provides web UI and CLI tools for managing user application\nOpenShift and Kubernetes architecture illustration\n\nOn the below fig we can see the control plane that control de cluster, runs on CoreOS, and Node e Infra Pods to do the own work on OpenShift.\nWe can have storage on Ceph, Gluster or from vendor. These runs on Bare metal or on cloud\n\n\n\n\n\n\nDescribing Kubernetes Resource Types\n\nPods (po) : collection of containers that share resources\nServices (svc) : How pods talk with each other. Single IP/port combination that provides access to a pool of pods\nReplication Controllers (rc) : how pods are replicated into different notes\nPersistent Volumes (pv) : Define storage areas to be used by Kubernetes pods.\nPersistent Volume Claims (pvc) : Represent a request for storage by a pod.\nConfigMaps (cm) and Secrets : Contains a set of keys and values that can be used by other resources\n\nOpenShift Resource Types\n\nDeployment and Deployment Config (dc) : Both are the representation of a set of containers, it contains the configuration to be applied to all containers of each pod replica (images, tags, storage definitions, etc)\nBuild config (bc) : Process to be executed in the OpenShift project. A bc works together with a dc to provide a basic CI/CD workflows.\nRoutes : Represent a DNS host name recognized by the OpenShift router as an ingress point for applications and microservices.\n\n\n\n6.6.2 Creating Kubernetes Resources\nThe main method to interacting with an RHOCP is using oc command line\noc &lt;command&gt;\n\noc login &lt;clusterURL&gt;\n\nThe pod resource definition syntax can be provided by JSON or YAML\nSample of YAML format\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wildfly\n  labels:\n    name: wildfly\nspec:\n  containers:\n    - resources:\n        limits:\n            cpu: 0.5\n      image: do276/todojee\n      name: wildfly\n      ports:\n          - containerPort: 8080\n            name: wildfly\n      env:\n          - name: MYSQL_ENV_MYSQL_DATABASE\n            value: items\n          - name: MYSQL_ENV_MYSQL_USER\n            value: user1\n          - name: MYSQL_ENV_MYSQL_PASSWORD\n            value: mypa55\nDiscovering services\nUsing IP and port :\n\nSVC_NAME_SERVICE_HOST is the service IP address.\nSVC_NAME_SERVICE_PORT is the service TCP port.\n\nUsing DNS:\n\nSVC_NAME .PROJECT_NAME.svc.cluster.local\nTo create a tunel to our machine\n\n[user@host ~]$ oc port-forward mysql-openshift-1-glqrp 3306:3306\n\nCreating application\n# sample 1\n[user@host ~]$ oc new-app mysql MYSQL_USER=user MYSQL_PASSWORD=pass MYSQL_DATABA SE=testdb -l db=mysql\n\n\nCreate using docker image\n\n# sample 2 using docker img\noc new-app --docker-image=myregistry.com/mycompany/myapp --name=myapp\n\nCreate using Git Repo\n\n# sample 3 from Git\noc new-app https://github.com/openshift/ruby-hello-world --name=ruby-hello\n\nCreating from template\n\n# from template\n$ oc new-app \\\n--template=mysql-persistent \\\n-p MYSQL_USER=user1 -p MYSQL_PASSWORD=mypa55 -p MYSQL_DATABASE=testdb \\\n-p MYSQL_ROOT_PASSWORD=r00tpa55 -p VOLUME_CAPACITY=10Gi\n...output omitted..\nManaging Persistent Storage\n\nList persistent volume objects\n\n[admin@host ~]$ oc get pv\n\nSee the YAML definition for a PersistentVolume\n\n[admin@host ~]$ oc get pv pv0001 -o yaml\n\nAdd more PersistentVolume objects\n\n[admin@host ~]$ oc create -f pv1001.yaml\n\nRequest persistent volume\n\nCreate a pvc object request :\n\nWe create a PersistentVolumeClaim (PVC) object to request :\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myapp\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nTo create the PVC :\n\n[admin@host ~]$ oc create -f pvc.yaml\n\nTo list the PVCs\n\n[admin@host ~]$ oc get pvc\n\nManaging OpenShift Resources at the command line\nThe oc get RESOURCE_TYPE command display the summary of all resources\n\noc get all : retrieve a summary of the most important components\noc describe RESOURCE_TYPE|RESOURCE_NAME : retrieve additional information\noc get RESOURCE_TYPE|RESOURCE_NAME : export a resource definition.\noc create RESOURCE_TYPE|RESOURCE_NAME : create resources\noc edit: edit resource definitions\noc delete RESOURCE_TYPE : remove resource from openshift\noc exec CONTAINER_ID : executes commands inside a container.\n\nTip :\nIf we use label we can make reference by label\n$ oc get svc,deployments -l app=nexus\n\n\n6.6.3 Creating Routes\nA route connects a public-facing IP address and DNS host name to an internal-facing service IP. It uses the service resource to find the endpoints; that is, the ports exposed by the service.\nOpenShift routes are implemented by a cluster-wide router service, sample of route defined using JSON:\n{\n  \"apiVersion\": \"v1\",\n  \"kind\": \"Route\",\n  \"metadata\": {\n      \"name\": \"quoteapp\"\n  },\n  \"spec\": {\n        \"host\": \"quoteapp.apps.example.com\",\n        \"to\": {\n              \"kind\": \"Service\",\n              \"name\": \"quoteapp\"\n        }\n    }\n}\n\nCreating routes\n\n$ oc expose service quotedb --name quote\n\nInspect the routes\n\n$ oc get pod --all-namespaces | grep router\n&gt; openshift-ingress router-default-746b5cfb65-f6sdm 1/1 Running 1 4d\n\n# describe :\n$ oc describe pod router-default-746b5cfb65-f6sdm\n\n\n6.6.4 Creating Applications with Source-to-Image\n\nSource-to-Image (S2I) : This tool takes an application’s source code from a Git repository, injects the source code into a base container and produces a new container image that runs the assembled application.\n\nImage stream resource is configuration that names specific container images to get the list of default images streams populated by OpenShift\n$ oc get is -n openshift\n\n\nBuyild an application with S2I and CLI\n\nFrom Git\n\n$ oc new-app php~http://my.git.server.com/my-app\n\n# or another syntax\n$ oc new-app -i php http://services.lab.example.com/app --name=myapp\n\n\nFrom path\n\noc new-app .\n\nUsing contect\n\n$ oc new-app https://github.com/openshift/sti-ruby.git --context-dir=2.0/test/puma-test-app\n\nSpecific branch\n\n$ oc new-app https://github.com/openshift/ruby-hello-world.git#beta4\n\nAfter creating a new application, the build process starts we can check using get builds command\n$ oc get builds\n\nCheck the logs\n\n$ oc logs build/myapp-1\n\nTrigger a new build with the oc start-build build_config_name command\n\n$ oc get buildconfig\n\n$ oc start-build myapp"
  },
  {
    "objectID": "OpenShift1.html#deploying-multi-container-applications",
    "href": "OpenShift1.html#deploying-multi-container-applications",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.7 Deploying Multi-Container Applications",
    "text": "6.7 Deploying Multi-Container Applications\n\n6.7.1 Considerations for Multi-Container Applications\n\nComplex applications have different components such as fron-end, REST back end and database server, it is possible to orchestrate multi-conatiner applications manually, but openshift and kubernets provide tools to facilite that.\n\n\n\n6.7.2 Deploying A Multi-Container App on OpenShift\n\nPods are attached to a Kubernetes namespace, which OpenShift calls a project\nWhen a pod starts, Kubernetes automatically adds a set of environment variables for each service defined on the same namespace.\nAny service defined on Kubernetes generates environment variables for the IP address and port number where the service is available\nKubernetes automatically injects these environment variables into the containers from pods in the same namespace\n&lt;SERVICE_NAME&gt;_SERVICE_HOST: Represents the IP address enabled by a service to accessa pod.\n&lt;SERVICE_NAME&gt;_SERVICE_PORT: Represents the port where the server port is listed.\n&lt;SERVICE_NAME&gt;_PORT: Represents the address, port, and protocol provided by the service for external access.\n&lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;: Defines an alias for the &lt;SERVICE_NAME&gt;_PORT.\n&lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;_PROTO: Identifies the protocol type (TCP or UDP).\n&lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;_PORT: Defines an alias for &lt;SERVICE_NAME&gt;_SERVICE_PORT. • &lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;_ADDR: Defines an alias for &lt;SERVICE_NAME&gt;_SERVICE_HOST.\n\n\n\n6.7.3 Deploying a Multi-container Application on OpenShift Using a Template\nA template defines a set of related resources to be created together, as well as a set of application parameters.\nThe OpenShift installer creates several templates by default in the openshift namespace.\n[user@host ~]$ oc get templates -n openshift\n\nCheck the YAML definition of template\n\n[user@host ~]$ oc get template mysql-persistent -n openshift -o yaml\n\nAssuming the template is defined in the todo-template.yaml file, use the oc create command to publish the application template:\n\n[user@host deploy-multicontainer]$ oc create -f todo-template.yaml\n\nlist available parameters from a template.\n\n[user@host ~]$ oc describe template mysql-persistent -n openshift\n\nor\n\n[user@host ~]$ oc process --parameters mysql-persistent -n openshift\n\nProcessing a Template Using the CLI\n\n[user@host ~]$ oc process -o yaml -f &lt;filename&gt;\n\nTemplates often generate resources with configurable attributes that are based on the template parameters. To override a parameter, use the -p option followed by a = pair.\n\n[user@host ~]$ oc process -o yaml -f mysql.yaml -p MYSQL_USER=dev -p MYSQL_PASSWORD=$P4SSD -p MYSQL_DATABASE=bank -p VOLUME_CAPACITY=10Gi &gt; mysqlProcessed.yaml\n\nTo create an application use the generated YAML\n\n[user@host ~]$ oc create -f mysqlProcessed.yaml\n\nAlternatively, it is possible to process the template and create the application without saving a resource definition file by using a UNIX pipe:\n\n[user@host ~]$ oc process -f mysql.yaml -p MYSQL_USER=dev  -p MYSQL_PASSWORD=$P4SSD -p MYSQL_DATABASE=bank  -p VOLUME_CAPACITY=10Gi | oc create -f -"
  },
  {
    "objectID": "OpenShift1.html#troubleshooting-containerized-applications",
    "href": "OpenShift1.html#troubleshooting-containerized-applications",
    "title": "6  Red Hat OpenShift I: Containers & Kubernetes - DO180",
    "section": "6.8 Troubleshooting Containerized Applications",
    "text": "6.8 Troubleshooting Containerized Applications\n\n6.8.1 Troubleshooting S2I Builds and Deployments\nIntroduction to the S2I Process\nS2I is simple way to create image, however problems can heppens is important to keep in mind the workflow for most of the program languages :\n\nBuild step :\n\ncompiling source code, packaging the application as a container image\npush the image to the OpenShift registry for deployment step\nBC (BuildConfig) drive the build step\n\nDeployment step :\n\nstarting a pod and making the application available\nexecutes after the build step\n\nCheck the logs\n\n$ oc logs bc/&lt;application-name&gt;\n\nrequest a new build:\n\n$ oc start-build &lt;application-name&gt;\n\nCheck the deployment logs\n\n$ oc logs deployment/&lt;application-name&gt;\n\nCommon Problems\nThe oc logs command provides important information about the build, deploy, and run processes of an application during the execution of a pod. The logs may include : * missing values * options that must be enabled * incorrect parameters or flags * environment incompatibilities,etc\n\n\n6.8.2 Troubleshooting Containerized Applications\nSometimes sysadmin need special access to container for example the dba need access the database container to check the database or sysadmin need to restart an specific service on a container, podman provides port forwarding features by using -p option with the `run`` subcommand\n$ podman run --name db -p 30306:3306 mysql\nOpenShift provide the oc port-foward command for fowarding a local port to a pod port.\n$ oc port-forward db 30306 3306\n\nTo access the container logs\n\n$ podman logs &lt;containerName&gt;\n\nTo access openshift logs\n\n$ oc logs &lt;podName&gt;\n\nTo read openshift events\n\n$ oc get events\n\nAccessing Running Container\n\n$ podman exec [options] container command [arguments\n\n$ oc exec [options] pod [-c container] -- command [arguments]\n\n# sample\n$ oc exec -it myhttpdpod /bin/bash\n$ podman exec apache-container cat /var/log/httpd/error_log\n\nTransfer file to and from containers\n\n$ podman run -v /conf:/etc/httpd/conf -d do180/apache\n\n$ podman cp standalone.conf todoapi:/opt/jboss/standalone/conf/standalone.conf\n$ podman cp todoapi:/opt/jboss/standalone/conf/standalone.conf ."
  }
]